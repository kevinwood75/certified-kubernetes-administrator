
#####Application Falure#########

* kubectl logs web -f  -> current live logs
* kubectl logs web -f --previous -> previvious logs

* Troubleshooring app pods
https://kubernetes.io/docs/tasks/debug/debug-application/debug-pods/


1) app cannot connect to DB service

* issue what mysql service name app pod used did not exist
* deleted current mysql service and deploy new server with name mysql-service
- controlplane /tmp ➜  kubectl edit svc mysql  -n alpha
A copy of your changes has been stored to "/tmp/kubectl-edit-3319683989.yaml"
error: At least one of apiVersion, kind and name was changed
- controlplane /tmp ✖ kubectl delete svc mysql -n alpha
service "mysql" deleted
controlplane /tmp ✖ kubectl apply -f /tmp/kubectl-edit-3319683989.yaml

2) getting connection to mysql refused from app
* mysql service was was confirgured with wrong target port
* kubectl edit svc mysql-service -n beta -> change to correct taget port 3306
*** end point has port in it now
controlplane ~ ➜  kubectl describe svc mysql-service  -n beta
Name:                     mysql-service
Namespace:                beta
Labels:                   <none>
Annotations:              <none>
Selector:                 name=mysql
Type:                     ClusterIP
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                       10.43.183.16
IPs:                      10.43.183.16
Port:                     <unset>  3306/TCP
TargetPort:               3306/TCP
Endpoints:                10.42.0.11:3306
Session Affinity:         None
Internal Traffic Policy:  Cluster
Events:                   <none>

3) getting connection refused to mysql from app
* issue is that mysqql service does not have the correct selector that matches label mysql pod is using.
kubectl edit svc mysql-service -n gamma

controlplane ~ ➜  kubectl describe svc mysql-service -n gamma
Name:                     mysql-service
Namespace:                gamma
Labels:                   <none>
Annotations:              <none>
Selector:                 name=mysql
Type:                     ClusterIP
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                       10.43.30.204
IPs:                      10.43.30.204
Port:                     <unset>  3306/TCP
TargetPort:               3306/TCP
Endpoints:                10.42.0.13:3306
Session Affinity:         None
Internal Traffic Policy:  Cluster
Events:                   <none>

4)  getting access denied talking to the dataabase .
* issue is the environment variable defining DB_USER in webapp pod is not using correct username
kubectl edit deployment webapp-mysql -n delta

controlplane ~ ➜  kubectl describe deployment webapp-mysql -n delta
Name:                   webapp-mysql
Namespace:              delta
CreationTimestamp:      Mon, 23 Dec 2024 21:23:04 +0000
Labels:                 name=webapp-mysql
Annotations:            deployment.kubernetes.io/revision: 2
Selector:               name=webapp-mysql
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  name=webapp-mysql
  Containers:
   webapp-mysql:
    Image:      mmumshad/simple-webapp-mysql
    Port:       8080/TCP
    Host Port:  0/TCP
    Environment:
      DB_Host:      mysql-service
      DB_User:      root
      DB_Password:  paswrd


5) getting access denied in talking to mysql from webapp
** 2 things wrong
* db_user was not set to correct user in web app pod environment variables
* evironment variable setting password on mysql pod was not correct
Environment:
      MYSQL_ROOT_PASSWORD:  paswrd

kubectl edit deployments -n epsilon -> change db-user on webapp
kubectl edit pod mysql -n epsilon
kubectl apply -f /tmp/kubectl-edit-1969819909.yaml

6) cannot reach webapp ui
* using wrong node port in webapp-service
** after being able to connect to webapp was getting access denied talking to database
kubectl edit deployments -n zeta
deployment.apps/webapp-mysql edited


controlplane ~ ➜  kubectl edit pod mysql -n zeta
error: pods "mysql" is invalid
A copy of your changes has been stored to "/tmp/kubectl-edit-693358918.yaml"
error: Edit cancelled, no valid changes were saved.

controlplane ~ ✖ kubectl delete pod mysql -n zeta
pod "mysql" deleted

controlplane ~ ➜  kubectl apply -f /tmp/kubectl-edit-693358918.yaml -n zeta
pod/mysql created


*************Control Plane Failure***************

* check pods in kube-system namespace are running
kubectl get pods -n kube-system

* if control plane is not running in a container check the services
*** 
Master:
service kube-apiserver status
service kube-controller-manager status
service kube-scheduler status
Worker:
service kubelet status
service kube-proxy status

* check the service logs 
** Kubeadm coponents are in Containers
kubectl logs kube-apiserver-master -n kube-system
** for services installed on control plane host look at logs of service
sudo journalctl -u kube-apiserver

https://kubernetes.io/docs/tasks/debug-application-cluster/debug-cluster/


* app not deploying all replica sets when deployed
** check the control-plan services
controlplane ~ ➜  kubectl get pods -n kube-system
NAME                                   READY   STATUS             RESTARTS      AGE
coredns-77d6fd4654-gjt5g               1/1     Running            0             6m7s
coredns-77d6fd4654-trjjz               1/1     Running            0             6m7s
etcd-controlplane                      1/1     Running            0             6m12s
kube-apiserver-controlplane            1/1     Running            0             6m12s
kube-controller-manager-controlplane   1/1     Running            0             6m12s
kube-proxy-jftvt                       1/1     Running            0             6m8s
kube-scheduler-controlplane            0/1     CrashLoopBackOff   4 (57s ago)   2m26s

** Look at details of kube-scheduler-controlrplane
kubectl describe pod kube-scheduler-controlplane -n kube-system
Name:                 kube-scheduler-controlplane
Namespace:            kube-system

Events:
  Type     Reason   Age                     From     Message
  ----     ------   ----                    ----     -------
  Warning  BackOff  2m33s (x10 over 3m48s)  kubelet  Back-off restarting failed container kube-scheduler in pod kube-scheduler-controlplane_kube-system(3aa60fbba62f9faa79076d6a3f6cb9ba)
  Normal   Pulled   2m22s (x5 over 3m50s)   kubelet  Container image "registry.k8s.io/kube-scheduler:v1.31.0" already present on machine
  Normal   Created  2m22s (x5 over 3m50s)   kubelet  Created container kube-scheduler
  Warning  Failed   2m21s (x5 over 3m50s)   kubelet  Error: failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: exec: "kube-schedulerrrr": executable file not found in $PATH: unknown

  ** fix scheduler yaml and re-deploy scheduler
  cd /etc/kubernetes/manifests
  vi kube-scheduler.yaml 
  kubectl apply -f kube-scheduler.yaml -n kube-system
  kubectl describe deployment app -> check for no errors

* increase replocas for deployment to 2
  kubectl scale deploy app --replicas=2


* not scaling up pods to match what replic set is set to

** control manger is what does the scale up
** contontroller manager in a failed state because conf file being used does 
not exist
ontrolplane / ➜  kubectl get pods -n kube-system
NAME                                   READY   STATUS             RESTARTS        AGE
coredns-77d6fd4654-gjt5g               1/1     Running            0               38m
coredns-77d6fd4654-trjjz               1/1     Running            0               38m
etcd-controlplane                      1/1     Running            0               38m
kube-apiserver-controlplane            1/1     Running            0               38m
kube-controller-manager-controlplane   0/1     CrashLoopBackOff   7 (4m39s ago)   15m
kube-proxy-jftvt                       1/1     Running            0               38m
kube-scheduler                         0/1     CrashLoopBackOff   9 (2m49s ago)   23m
kube-scheduler-controlplane            1/1     Running            0               27m

kubectl describe pod kube-controller-manager-controlplane -n kube-system
** did not find issue

**look at pod logs

controlplane / ➜  k logs kube-controller-manager-controlplane -n kube-system
I1229 16:26:41.188122       1 serving.go:386] Generated self-signed cert in-memory
E1229 16:26:41.188228       1 run.go:72] "command failed" err="stat /etc/kubernetes/controller-manager-XXXX.conf: no such file or directory"
** conf file does not exist need to correct it

**
controlplane /etc/kubernetes/manifests ➜  vi kube-controller-manager.yaml
--kubeconfig=/etc/kubernetes/controller-manager-XXXX.conf -> change to correct file
**


* was not scaling pods to 3 after being scaled up
** controlmanger controls scaling.
controlplane /etc/kubernetes/manifests ➜  k logs kube-controller-manager-controlplane -n kube-system
I1230 16:23:38.503744       1 serving.go:386] Generated self-signed cert in-memory
E1230 16:23:38.639199       1 run.go:72] "command failed" err="unable to load client CA provider: open /etc/kubernetes/pki/ca.crt: no such file or directory"
**complaing about ca.crt file loading
**certs are mounted from local FS using volume mounts -> file exits in /etc/kubernetes/pki
kubectl decribe pod kube-controller-manager-controlplane -> can see volume mount is pointing to directory that does not exist. swtich to correct directory to fix
**edit controller manger config in /etc/kubernetes/manifest/

**********************Worker node failures*********************
* check status on nodes in cluster do they report as READY
kubectl get nodes

* If they report as not ready use the bellow command
kubectl describe node worker-1
** each node has a set of conditions to check health either set to true.false or unknown

* if work node conditions are set to unknown it means its not talking to controller
* check health of worker node (cpu,memory,disk.etc)
* check the kublet service and logs 
* check kubelet certificates that they have not expired and are part of correct group and issues by the correct CA
openssl x509 -in /var/lib/kubelet/worker-1.crt -text

* check kubelet logs on worker node.
journalctl -u kubelet

* kublet not able to talk to controller and service is running 
** seeing connection issue in kubelet service talking to api on controller
** fix port /etc/kubernetes/kubelet.conf
** restart kubelet service on worker


******************************Network troubleshooting*******************
Network Troubleshooting
Network Plugin in Kubernetes
--------------------

There are several plugins available and these are some.



1. Weave Net:



To install,



kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml



You can find details about the network plugins in the following documentation :

https://kubernetes.io/docs/concepts/cluster-administration/addons/#networking-and-network-policy



2. Flannel :



 To install,

 

kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/2140ac876ef134e0ed5af15c65e414cf26827915/Documentation/kube-flannel.yml

   

Note: As of now flannel does not support kubernetes network policies.



3. Calico :

   

   To install,

   curl https://raw.githubusercontent.com/projectcalico/calico/v3.25.0/manifests/calico.yaml -O

  Apply the manifest using the following command.

      kubectl apply -f calico.yaml


   Calico is said to have most advanced cni network plugin.



In CKA and CKAD exam, you won't be asked to install the CNI plugin. But if asked you will be provided with the exact URL to install it.

Note: If there are multiple CNI configuration files in the directory, the kubelet uses the configuration file that comes first by name in lexicographic order.





DNS in Kubernetes
-----------------
Kubernetes uses CoreDNS. CoreDNS is a flexible, extensible DNS server that can serve as the Kubernetes cluster DNS.



Memory and Pods

In large scale Kubernetes clusters, CoreDNS's memory usage is predominantly affected by the number of Pods and Services in the cluster. Other factors include the size of the filled DNS answer cache, and the rate of queries received (QPS) per CoreDNS instance.



Kubernetes resources for coreDNS are:   

a service account named coredns,

cluster-roles named coredns and kube-dns

clusterrolebindings named coredns and kube-dns, 

a deployment named coredns,

a configmap named coredns and a

service named kube-dns.



While analyzing the coreDNS deployment you can see that the the Corefile plugin consists of important configuration which is defined as a configmap.



Port 53 is used for for DNS resolution.



    kubernetes cluster.local in-addr.arpa ip6.arpa {
       pods insecure
       fallthrough in-addr.arpa ip6.arpa
       ttl 30
    }


This is the backend to k8s for cluster.local and reverse domains.



proxy . /etc/resolv.conf



Forward out of cluster domains directly to right authoritative DNS server.





Troubleshooting issues related to coreDNS
1. If you find CoreDNS pods in pending state first check network plugin is installed.

2. coredns pods have CrashLoopBackOff or Error state

If you have nodes that are running SELinux with an older version of Docker you might experience a scenario where the coredns pods are not starting. To solve that you can try one of the following options:

a)Upgrade to a newer version of Docker.

b)Disable SELinux.

c)Modify the coredns deployment to set allowPrivilegeEscalation to true:



kubectl -n kube-system get deployment coredns -o yaml | \
  sed 's/allowPrivilegeEscalation: false/allowPrivilegeEscalation: true/g' | \
  kubectl apply -f -
d)Another cause for CoreDNS to have CrashLoopBackOff is when a CoreDNS Pod deployed in Kubernetes detects a loop.



  There are many ways to work around this issue, some are listed here:



Add the following to your kubelet config yaml: resolvConf: <path-to-your-real-resolv-conf-file> This flag tells kubelet to pass an alternate resolv.conf to Pods. For systems using systemd-resolved, /run/systemd/resolve/resolv.conf is typically the location of the "real" resolv.conf, although this can be different depending on your distribution.

Disable the local DNS cache on host nodes, and restore /etc/resolv.conf to the original.

A quick fix is to edit your Corefile, replacing forward . /etc/resolv.conf with the IP address of your upstream DNS, for example forward . 8.8.8.8. But this only fixes the issue for CoreDNS, kubelet will continue to forward the invalid resolv.conf to all default dnsPolicy Pods, leaving them unable to resolve DNS.



3. If CoreDNS pods and the kube-dns service is working fine, check the kube-dns service has valid endpoints.

              kubectl -n kube-system get ep kube-dns

If there are no endpoints for the service, inspect the service and make sure it uses the correct selectors and ports.





Kube-Proxy
---------
kube-proxy is a network proxy that runs on each node in the cluster. kube-proxy maintains network rules on nodes. These network rules allow network communication to the Pods from network sessions inside or outside of the cluster.



In a cluster configured with kubeadm, you can find kube-proxy as a daemonset.



kubeproxy is responsible for watching services and endpoint associated with each service. When the client is going to connect to the service using the virtual IP the kubeproxy is responsible for sending traffic to actual pods.



If you run a kubectl describe ds kube-proxy -n kube-system you can see that the kube-proxy binary runs with following command inside the kube-proxy container.



    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
 

    So it fetches the configuration from a configuration file ie, /var/lib/kube-proxy/config.conf and we can override the hostname with the node name of at which the pod is running.

 

  In the config file we define the clusterCIDR, kubeproxy mode, ipvs, iptables, bindaddress, kube-config etc.

 

Troubleshooting issues related to kube-proxy
1. Check kube-proxy pod in the kube-system namespace is running.

2. Check kube-proxy logs.

3. Check configmap is correctly defined and the config file for running kube-proxy binary is correct.

4. kube-config is defined in the config map.

5. check kube-proxy is running inside the container

# netstat -plan | grep kube-proxy
tcp        0      0 0.0.0.0:30081           0.0.0.0:*               LISTEN      1/kube-proxy
tcp        0      0 127.0.0.1:10249         0.0.0.0:*               LISTEN      1/kube-proxy
tcp        0      0 172.17.0.12:33706       172.17.0.12:6443        ESTABLISHED 1/kube-proxy
tcp6       0      0 :::10256                :::*                    LISTEN      1/kube-proxy




References:

Debug Service issues:

                     https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/

DNS Troubleshooting:

                     https://kubernetes.io/docs/tasks/administer-cluster/dns-debugging-resolution/



