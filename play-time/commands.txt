
testing
kubernetes documentation:
https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/

https://kubernetes.io/docs/home/


kubectl create -f pod-definition.yml

Kevins-MacBook-Air:play-time kwood$ kubectl get pods
NAME                    READY   STATUS    RESTARTS      AGE
myapp-pod               1/1     Running   0             8s
nginx-55f598f8d-h9md4   1/1     Running   1 (47m ago)   32d

kubectl describe pod myapp-pod


evins-MacBook-Air:play-time kwood$ kubectl get replicationcontroller
NAME       DESIRED   CURRENT   READY   AGE
myapp-rc   3         3         3       49s


kubectl replace -f replicaset-definition.yaml

Kevins-MacBook-Air:play-time kwood$ kubectl get replicaset
NAME               DESIRED   CURRENT   READY   AGE
myapp-replicaset   6         6         6       5m55s

kubectl scale --replicas=3 replicaset myapp-replicaset
kubectl scale --replicas=3 -f replicaset-definition.yaml

kubectl edit rs new-replica-set

specify namespace with creating pod:

kubectl create namespace dev

kubectl create -f pod-definition.yml --namespace=dev

kubectl get pods --namespace=dev



kubectl replace --force -f /tmp/test.yaml  -> kill and rebuilding existing pod using the new definition




Switch to another namespace:
kubectl config set-context $(kubectl config current-context) --namespace=dev

View pods in all namespaces:
kubectl get pods --all-namespaces

Imperative:

kubectl replace -f nginx.yaml   -> to update an object
kubectl replace --force -f nginx.yaml  -> delets object and then recreates it.

Create an NGINX Pod
kubectl run nginx-pod --image=nginx:alpine
kubectl run httpd --image=httpd:alpine --port=80 --expose=true


Generate POD Manifest YAML file (-o yaml). Don't create it(--dry-run)
kubectl run nginx --image=nginx --dry-run=client -o yaml
kubectl run redis --image=redis:alpine --dry-run=client -o yaml

Create a deployment
kubectl create deployment --image=nginx nginx
kubectl run custom-nginx --image=nginx --port=8080


Generate Deployment YAML file (-o yaml). Don't create it(--dry-run)
kubectl create deployment --image=nginx nginx --dry-run=client -o yaml


Generate Deployment with 4 Replicas
kubectl create deployment nginx --image=nginx --replicas=4

kubectl create deployment webapp --image=kodekloud/webapp-color --replicas=3

You can also scale a deployment using the kubectl scale command.
kubectl scale deployment nginx --replicas=4

Another way to do this is to save the YAML definition to a file and modify
kubectl create deployment nginx --image=nginx --dry-run=client -o yaml > nginx-deployment.yaml


Create a Service named redis-service of type ClusterIP to expose pod redis on port 6379
kubectl expose pod redis --port=6379 --name redis-service --dry-run=client -o yaml
kubectl expose pod custom-nginx --port=8080 --name custom-nginx-service --dry-run=client -o yaml


kubectl create service clusterip redis --tcp=6379:6379 --dry-run=client -o yaml 

(This will not use the pods labels as selectors, 
instead it will assume selectors as app=redis. You cannot pass in selectors as an option. So it does not work very well 
if your pod has a different label set. So generate the file and modify the selectors before creating the service)


Create a Service named nginx of type NodePort to expose pod nginx's port 80 on port 30080 on the nodes:

kubectl expose pod nginx --type=NodePort --port=80 --name=nginx-service --dry-run=client -o yaml

(This will automatically use the pod's labels as selectors, but you cannot specify the node port. 
You have to generate a definition file and then add the node port in manually before creating the service with the pod.)


kubectl create service nodeport nginx --tcp=80:80 --node-port=30080 --dry-run=client -o yaml

(This will not use the pods labels as selectors)

Both the above commands have their own challenges. While one of it cannot accept a selector the other cannot 
accept a node port. I would recommend going with the kubectl expose command. If you need to specify a node port, 
generate a definition file using the same command and manually input the nodeport before creating the service.


Reference:
https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands

https://kubernetes.io/docs/reference/kubectl/conventions/


Declaritive

kubectl apply -f nginx.yml   -> wiill update and create object if it does not exist
kubectl apply -f /path/to/config-files   -> can point at directory and it will apply all *.yml in that directory


https://kubernetes.io/docs/reference/kubectl/conventions/

Create an NGINX Pod

kubectl run nginx --image=nginx

Generate POD Manifest YAML file (-o yaml). Don't create it(--dry-run)

kubectl run nginx --image=nginx --dry-run=client -o yaml

Create a deployment

kubectl create deployment --image=nginx nginx

Generate Deployment YAML file (-o yaml). Don't create it(--dry-run)

kubectl create deployment --image=nginx nginx --dry-run=client -o yaml

Generate Deployment YAML file (-o yaml). Don’t create it(–dry-run) and save it to a file.

kubectl create deployment --image=nginx nginx --dry-run=client -o yaml > nginx-deployment.yaml

Make necessary changes to the file (for example, adding more replicas) and then create the deployment.

kubectl create -f nginx-deployment.yaml



OR

In k8s version 1.19+, we can specify the --replicas option to create a deployment with 4 replicas.

kubectl create deployment --image=nginx nginx --replicas=4 --dry-run=client -o yaml > nginx-deployment.yaml


-----------------------------Scheduler---------------------------------

Manually:

Run the command: kubectl get pods --namespace kube-system
to see the status of scheduler pod. We have removed the scheduler from this Kubernetes cluster. 
As a result, as it stands, the pod will remain in a pending state forever.

controlplane ~ ➜  more nginx.yaml 
---
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  -  image: nginx
     name: nginx
  nodeName: controlplane


Labels and Selectors:

  kubectl get pods --selector app=App1

  list all objects:
  kubectl get all --selector env=prod --no-headers | wc -l

  controlplane ~ ➜  kubectl get all --selector env=prod,bu=finance,tier=frontend
NAME              READY   STATUS    RESTARTS   AGE
pod/app-1-zzxdf   1/1     Running   0          3m11s

Taints and Tolerations:

Taints are added to nodes:

kubectl taint nodes node-name key=value:taint-effect

What happens to PODs that Do not tolerate this taint  NoSchedule,PreferNoSchedule,NoExecute

kubectl taint nodes node1 app=blue:NoSchedule


Tolerations are added to pods

pod definition under spec:

tolerations:
- key:"app"
  operator:"Equal"
  value:"blue"
  effect:"NoSchedule"


to see the tain on the master node
kubectl describe node kubemastrer | grep Taint

Scheduler:

you can specify nodeName to tell pod where to run if no scheduler
pod-def.yaml add nodeName: host to spec section

other option when no scheduler:

create a pod-bind-definition.yaml:
apiVersion: v1
kind: Binding
metadata:
  name: nginx
target:
  apiVersion: v1
  kind: Node
  name: node02


  conver the yaml to json format and send to apiVersion
  curl --header "Content-Type:application/json" --request POST --data '{}' http://$SERVER/api/v1/namespaces/default/pods/$PODNAME/binding/


  Node Selectors
  poddef.yaml in spec section:
  nodeSelector:
    size: Large

  Labelnodes:
  kubectl label node <node-name>  size=Large

  kubectl create deployment blue --image=nginx --replicas=3

Check for taints:

kubectl describe node node01 | grep Taints

Edit a deployment:

kubectl edit deployment deploymentname

create yaml for a delpoyment:
kubectl create deployment red --image=nginx --replicas=2 --dry-run=client -o yaml > red.yaml



  NODE AFFINITY
  pod-def.yaml  spec section:

  affinity:
   nodeAffinity:
     requiredDuringScheduleingIgnoreDuringExecution:
       nodeSelectorTerms:
       - matchExpressions:
         - key: size
           operator: In or NotIn or Exists
           values:
           - Large

Resource Requirements and Limits:

resources:
  requests:
    memory: "4Gi"
    cpu: 2

1 cpu count is the same as AWS vCPU or GCP/Azure core , hyperthread

1 K(Kilobyte) = 1,000 byte
1 Ki(Kibibyte) = 1,024 byte

Limit Ranges,

sets global resource limits for all containers if none are sepecified at namespace level

limit-range-cpu.yaml

apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-resource-constraint
spec:
  limits:
  - default:
      cpu: 500m
    defaultRequest:
      cpu: 500m
    max:
      cpu: "1"
    min:
      cpu: 100m
    type: Container

limit-range-memory.yaml

apiVersion: v1
kind: LimitRange
metadata:
  name: memory-resource-constraint
spec:
  limits:
  - default:
      memory: 1Gi
    defaultRequest:
      memory: 1Gi
    max:
      memory: 1Gi
    min:
      memory: 500Mi
    type: Container

Creating resource Quotas at a namespace level:
ResourceQuota -> is a namespace object

resource-quota.yaml

apiVersion: v1
kind: ResourceQuota
metadata
  name: my-resource-quota
spec:
  hard:
    requests.cpu: 4
    requests.memory: 4Gi
    limits.cpu: 10
    limits.memory: 10Gi

    Manage Memory,CPU and API Resources:
    https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/

    LimitRange for CPU:
    https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/

    LimitRange for Memory:
    https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/

Remember, you CANNOT edit specifications of an existing POD other than the below.

spec.containers[*].image

spec.initContainers[*].image

spec.activeDeadlineSeconds

spec.tolerations


With Deployments you can easily edit any field/property of the POD template. Since the pod template
 is a child of the deployment specification,  with every change the deployment will automatically 
 delete and create a new pod with the new changes. So if you are asked to edit a property of a POD 
 part of a deployment you may do that simply by running the command




Daemon Sets: 

are like replicas but it runs one pod on each node of your cluster. when a new node is added to cluster
a pod is automatically spun up on new node.

Use cases of Daemon sets
- Monitoring Solution
- Logs Viewer


Creating a DaemonSet definition

daemon-set-definition.yaml

apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: monitoring-daemon
spec:
  selector:
     matchLabels:
       app: monitoring-agent
  template:
     metadata:
       labels:
         app: monitoring-agent
     spec:
       containers:
       - name: monitoring-agent
         image: monitoring-agent

kubectl get daemonsets --all-namespaces


Static pods:

These are pods are created on worker node only. by adding pod definition file to /etc/kubernetes/manifests directory.  
Kubelet reads this directory and creates pods based on definition files found automatically
/etc/kubernetes/manifests is passed when service starts using --pod-manifest-path=path

--config can be pointed ata config file also

kubeconfig.yaml
staticPodPath: /etc/kubernetes/manifests

kubectl get pods -A --no-headers | awk '{print $2 " -n " $1 "\n"}' | xargs --max-lines=1 kubectl describe pods | grep "Controlled By:  Node/controlplane" | wc -l

kubectl get pods -A --no-headers | awk '{print $2 " -n " $1 "\n"}' | xargs --verbose --max-lines=1 kubectl describe pods | grep "Controlled By:  Node/controlplane"

kubectl get pods --all-namespaces -o json | jq -r '.items | map(select(.metadata.ownerReferences[]?.kind == "Node" ) | .metadata.name) | .[]'




Scheduler:

kubectl get serviceaccount -n kube-system
kubectl get clusterrolebinding

https://github.com/kubernetes/community/blob/master/contributors/devel/sig-scheduling/scheduling_code_hierarchy_overview.md

https://kubernetes.io/blog/2017/03/advanced-scheduling-in-kubernetes/

https://jvns.ca/blog/2017/07/27/how-does-the-kubernetes-scheduler-work/

https://stackoverflow.com/questions/28857993/how-does-kubernetes-scheduler-work



Logging and monitoring

Metrics SERVER
kubectl top node

Kevins-MacBook-Air:cmd kwood$ kubectl top pod
NAME                     CPU(cores)   MEMORY(bytes)   
myapp-replicaset-g6zk9   0m           2Mi             
myapp-replicaset-lr5lm   0m           2Mi             
nginx                    0m           2Mi             
nginx-55f598f8d-h9md4    0m           2Mi     


deploy metrics server:

git clone https://github.com/kodekloudhub/kubernetes-metrics-server.git


viewing logs of a running pod:

kubectl logs -f pod_name container_name


###########Application Lifecycle Management###############

kubectl rollout status deployment/myapp-deployment  -> look at rollout

kubectl rollout history deployment/myapp-deployment


Deployment stategies:
recreate: teardown and build new
rolling update: takedown and build one at a time in a rolling fashion

During a rolling update deployment it will create a new replica set spinning down old and spinning up in new.


undo and upgrade:
kubectl rollout undo deployment/myapp-deployment -> destroys pods in new replica set and brings the old one up.


kubectl create -f deployment-definition.yaml
kubectl get deployments
kubectl apply -f deployment-definition.yaml
kubectl set image deployment/myapp-deployment nginx=nginx:1.9.1  -> nginx is the container name
kubectl rollout status deployment/myapp-deployment
kubectl rollout history deployment/myapp-deployment
kubectl rollout undo deployment/myapp-deployment


---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
  namespace: default
spec:
  replicas: 4
  selector:
    matchLabels:
      name: webapp
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        name: webapp
    spec:
      containers:
      - image: kodekloud/webapp-color:v2
        name: simple-webapp
        ports:
        - containerPort: 8080
          protocol: TCP


commands in containers docker:

docker run ubuntu   -> start ubuntu container then exits
docker ps -a -> lists all containers even the stopped ones.

Dockerfile:
run directive defines program that will run in container
CMD ["nginx"]

by default a terminal is not attached when a command is range

specify a command to run:
docker run ubuntu sleep 5

CMD ["command", "param1"]  -> command line replaced defined CMD

ENTRYPOINT instruction specifys what program will be run at start and what ever you specify at command line will be a 
paramater

FROM ubuntu
ENTRYPOINT ["sleep"]
CMD ["5]  -> define default sleep time if one is not specified in commandline

doecker run ubuntu-sleeper 10


you can override cmd in command line:
docker run --entrypoint sleep2.0 ubuntu-sleeper 10


create a pod with container(append args of container in pod)
pod-definition.yml

apiVersion: v1
kind: Pod
metadata:
 name: ubuntu-sleeper-pod
spec: 
 containers:
   - name: ubuntu-sleeper
     image: ubuntu-sleeper
     command: ["sleep2.0"]   --> overwrites the ENTRYPOINT
     args: ["10"]


Configmap:

kubectl create configmap <config-name> --from-literal=<key>=<value>

kubectl create configmap \
    app-config --from-literal=APP_COLOR=blue \
               --from-literal=APP_MOD=prod


kubectl create configmap
    <config-name> --from-file=<path-to-file>


config-map.yaml

apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_COLOR: blue
  APP_MODE: prod


kubectl get configmaps
kubectl describe configmaps

using configmap in pod-definition file under container section

envFrom: 
  - configMapRef:
        name: app-config





A note about Secrets!
Remember that secrets encode data in base64 format. Anyone with the base64 encoded secret can easily decode it. As such the secrets can be considered as not very safe.

The concept of safety of the Secrets is a bit confusing in Kubernetes. The kubernetes documentation page and a lot of blogs out there refer to secrets as a "safer option" to store sensitive data. They are safer than storing in plain text as they reduce the risk of accidentally exposing passwords and other sensitive data. In my opinion it's not the secret itself that is safe, it is the practices around it. 

Secrets are not encrypted, so it is not safer in that sense. However, some best practices around using secrets make it safer. As in best practices like:

Not checking-in secret object definition files to source code repositories.

Enabling Encryption at Rest for Secrets so they are stored encrypted in ETCD. 



Also the way kubernetes handles secrets. Such as:

A secret is only sent to a node if a pod on that node requires it.

Kubelet stores the secret into a tmpfs so that the secret is not written to disk storage.

Once the Pod that depends on the secret is deleted, kubelet will delete its local copy of the secret data as well.

Read about the protections and risks of using secrets here



Having said that, there are other better ways of handling sensitive data like passwords in Kubernetes, such as using tools like Helm Secrets, HashiCorp Vault. I hope to make a lecture on these in the future.



More on secrets:  https://www.youtube.com/watch?v=MTnQW9MxnRI

create a pod yaml file from scratch:

kubectl run yellow --image=busybox --dry-run=client -o yaml > yellow.yaml

Get logs from pod:  kubectl logs app -n elastic-stack
running conmmaand on container: kubectl -n elastic-stck exec -it app -- cat /log/app.log

sidecar:
https://kubernetes.io/docs/tasks/access-application-cluster/communicate-containers-same-pod-shared-volume/

https://bit.ly/2EXYdHf

add side care container to pod:

spec:
  containers:
  - image: kodekloud/filebeat-configured
    name: sidecar
    volumeMounts:
    - mountPath: /var/log/event-simulator/
      name: log-volume 


There are 3 common patterns, when it comes to designing multi-container PODs. The first and what we just saw with the logging 
service example is known as a side car pattern. The others are the adapter and the ambassador pattern.

InitContainers:


apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    command: ['sh', '-c', 'echo The app is running! && sleep 3600']
  initContainers:
  - name: init-myservice
    image: busybox
    command: ['sh', '-c', 'git clone <some-repository-that-will-be-used-by-application> ; done;']


But at times you may want to run a process that runs to completion in a container. For example a process that pulls a code or 
binary from a repository that will be used by the main web application. That is a task that will be run only  one time when the 
pod is first created. Or a process that waits  for an external service or database to be up before the actual application starts. 
That's where initContainers comes in.



https://kubernetes.io/docs/concepts/workloads/pods/init-containers/


rebuild bod with new definition:
kubectl replace --force -f /tmp/file

kubectl drain node-1 -> gracefully terminiated
kubectl cordon node-2 ->  un scheduleable.  meaning no pod can be scheduled on node
kubectl uncordon node-1 -> so nodes can be scheduled again


https://kubernetes.io/docs/concepts/overview/kubernetes-api/

Here is a link to kubernetes documentation if you want to learn more about this topic (You don't need it for the exam though):

https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md

https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api_changes.md


Backup Resource configs:
kubectl get all --all-namespaces -o yaml > all-deploy-services.yaml

velero to backup kubernetes cluster


Backup ETCD:
Look at "--data-dir" in startup and backup that location

snapshot ETCD:
etcdctl snapshot save snapshot.db

snapshot status:
etcdctl snapshot status snapshot.db

Restore from snapshot:
service kube-apiserver stop
etcdctl snapshot restore snapshot.db --data-dir /var/lib/etcd-from-backup
systemctl daemon-reload
service etcd restart
service kube-apiserver start

etcdctl is a command line client for etcd.



In all our Kubernetes Hands-on labs, the ETCD key-value database is deployed as a static pod on the master. The version used is v3.

To make use of etcdctl for tasks such as back up and restore, make sure that you set the ETCDCTL_API to 3.



You can do this by exporting the variable ETCDCTL_API prior to using the etcdctl client. This can be done as follows:

export ETCDCTL_API=3

On the Master Node:





To see all the options for a specific sub-command, make use of the -h or --help flag.



For example, if you want to take a snapshot of etcd, use:

etcdctl snapshot save -h and keep a note of the mandatory global options.



Since our ETCD database is TLS-Enabled, the following options are mandatory:

--cacert                                                verify certificates of TLS-enabled secure servers using this CA bundle

--cert                                                    identify secure client using this TLS certificate file

--endpoints=[127.0.0.1:2379]          This is the default as ETCD is running on master node and exposed on localhost 2379.

--key                                                      identify secure client using this TLS key file





Similarly use the help option for snapshot restore to see all available options for restoring the backup.

etcdctl snapshot restore -h

For a detailed explanation on how to make use of the etcdctl command line tool and work with the -h flags, check out the solution video for the Backup and Restore Lab.


Getting ETCD version:
k describe pod etcd-controlplane -n kube-system  -> look at image to get version

backup:
need to set "export ETCDCTL_API=3" before you can run "etcdctl snapshot"




127.0.0.1:2379
/etc/kubernetes/pki/etcd/server.crt
/etc/kubernetes/pki/etcd/ca.crt

Backup snapshot
etcdctl snapshot save --endpoints=127.0.0.1:2379  --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key /opt/snapshot-pre-boot.db

restore snapshot:
etcdctl snapshot restore --data-dir /var/lib/etcd-from-backup /opt/snapshot-pre-boot.db
vi /etc/kubernetes/manifests/etcd.yaml
Change host path to location where you restored:  -> once updated etcd will be reloaded.  

 - hostPath:
      path: /var/lib/etcd-from-backup
      type: DirectoryOrCreate
    name: etcd-data


show how many kubertnetes clusters:  k config view

switch between clusters:

student-node ~ ➜  kubectl config use-context cluster1
Switched to context "cluster1".

stacked etcd server is one that is running on controlplan and api service talks to it through 127.0.0.1

not stacked you will not see config in /etc/kubernetes/manifests/ on controlrplane

api service definition will not have ETCD-server being 127.0.0.1


listing etcd members:
etcdctl --endpoints=127.0.0.1:2379 --cacert=/etc/etcd/pki/ca.pem --cert=/etc/etcd/pki/etcd.pem --key=/etc/etcd/pki/etcd-key.pem member list
a18aeebcc70201bc, started, etcd-server, https://192.33.236.24:2380, https://192.33.236.24:2379, false



--advertise-client-urls=https://192.33.236.9:2379
--cert-file=/etc/kubernetes/pki/etcd/server.crt
--trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
--key-file=/etc/kubernetes/pki/etcd/server.key


https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster

https://github.com/etcd-io/website/blob/main/content/en/docs/v3.5/op-guide/recovery.md

https://www.youtube.com/watch?v=qRPNuT080Hk


Basic user auth with password:
curl -v -k https://master-node-ip:6443/api/v1/pods -u "user1:password123"

Token Auth:
curl -v -k https://master-node-ip:6443/api/v1/pods --header "Authorization: Bearer Kphkhjkhkjhuihjhjkhjkhjkhjkhjkhjk"

Article on Setting up Basic Authentication
Setup basic authentication on Kubernetes (Deprecated in 1.19)
Note: This is not recommended in a production environment. This is only for learning purposes. Also note that this approach is deprecated in Kubernetes version 1.19 and is no longer available in later releases

Follow the below instructions to configure basic authentication in a kubeadm setup.

Create a file with user details locally at /tmp/users/user-details.csv

# User File Contents
password123,user1,u0001
password123,user2,u0002
password123,user3,u0003
password123,user4,u0004
password123,user5,u0005


Edit the kube-apiserver static pod configured by kubeadm to pass in the user details. The file is located at /etc/kubernetes/manifests/kube-apiserver.yaml



apiVersion: v1
kind: Pod
metadata:
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
      <content-hidden>
    image: k8s.gcr.io/kube-apiserver-amd64:v1.11.3
    name: kube-apiserver
    volumeMounts:
    - mountPath: /tmp/users
      name: usr-details
      readOnly: true
  volumes:
  - hostPath:
      path: /tmp/users
      type: DirectoryOrCreate
    name: usr-details


Modify the kube-apiserver startup options to include the basic-auth file



apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    - --authorization-mode=Node,RBAC
      <content-hidden>
    - --basic-auth-file=/tmp/users/user-details.csv
Create the necessary roles and role bindings for these users:



---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups: [""] # "" indicates the core API group
  resources: ["pods"]
  verbs: ["get", "watch", "list"]
 
---
# This role binding allows "jane" to read pods in the "default" namespace.
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: read-pods
  namespace: default
subjects:
- kind: User
  name: user1 # Name is case sensitive
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role #this must be Role or ClusterRole
  name: pod-reader # this must match the name of the Role or ClusterRole you wish to bind to
  apiGroup: rbac.authorization.k8s.io
Once created, you may authenticate into the kube-api server using the users credentials

curl -v -k https://localhost:6443/api/v1/pods -u "user1:password123"


TLS  Certificates:

openssl genrsa -out my-bank.key 1024
-> my-bank.key
openssl rsa -in my-bank.key -pubout > mybank.pem
my-bank.key  mybank.pem

Public keys are *.pem or *.crt
private keys are *.key or *-key.pem

Generate Certs for kubernetes
CA:
Generate keys -> openssl genrsa -out ca.key 2048
Certificate Signing request -> openssl req -new -key ca.key -subj "/CN=KUBERNETES-CA" -out ca.creates
Sign Certificate -> openssl x509 -req -in ca.csr -signkey ca.key -out ca.crt

Client certs:

[Admin]
Generate keys -> opensssl genrsa -out admin.key 2048
Generate CSR -> openssl req -new key admin.key -subj "CN=Kube-admin" -out admin.csr
Sign Certs -> openssl x509 -req -in admin.csr -CA ca.crt -CAkey ca.key -out admin.crt




Server certs:
openssl genrsa -out apiserver.key 2048
openssl req -new -key apiserver.key -subj "/CN=kube-apiserver" -out apiserver.csr

create <<openssl.cnf>>
[req]
req_extensions = v3_req
distinguished_name = req_distinguished_name
[v3_req ]
basicConstraints = CA:false
keyUsage = nonRepudiation,
subjectAltName = @alt_names
[alt_names]
DNS.1 = kubernetes
DNS.2 = kubernetes.default
DNS.3 = kubernetes.default.service
DNS.4 = kubernetes.default.svc.cluster.local
IP.1 = 10.96.0.1
IP.2 = 172.17.0.87


openssl req -new -key apiserver.key -subj "/CN=kube-apiserver" -out apiserver.csr -config openssl.cnf
openssl x509 -req -in apiserver.csr -CA ca.crt -CAkey ca.key -out apiserver.crt


I have uploaded the Kubernetes Certificate Health Check Spreadsheet here:

https://github.com/mmumshad/kubernetes-the-hard-way/tree/master/tools


crictl ps -a


Certicates API

User access through certs:
openssl genrsa -out jane.key 2048
openssl req -new -key jane.key -subj "/CN=jane" -out jane.csr   -> cert signing request
Next create a certificate signing object:
<jane-csr.yaml>
apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: jane
spec:
  expirationSeconds: 600
  usages:
  - digital signature
  - key encipherment
  - server auth
  request:

       content of "cat jane.csr | base64
<>
kubectl create -f jane-csr.yaml

All certificate signing requests can be ssen by any admin "kubectl get csr"

another abme has to approve request:
kubectl certificate approve jane
kubernetes signs it .

Admin can extract user signed cert:
kubectl get csr jane -o yaml
echo "certiuficate: section" | base64 --decode 


All the certificate related operations are carried out by the Controller Manager


base64 -w 0  -> decodes with no line breaks

kubectl certificate deny agent-smith
kubectl delete csr agent-smith


defaults for kubectl command $HOME/.kube/config

kubectl config use-context research --kubeconfig /root/my-kube-config  -> using not default kubeconfig filemMy 


Kunernetes API:

curl http://localhost:6443 -K   -> will fail without creds
curl http://localhost:6443/apis -k | grep "name"  -> will fail withoiut creds

kubctl proxy -> launches a proxy 8001  using your kunconfig creds

using kubectl proxy you can access the kubernetes api without creds
curl http://localhost:8001 -K


kubectl proxy != kube proxy
Kubectl proxy is a http proxy service to access kubeapi
kubeproxy - allows communication bettween pods


Authorization:

RBAC  -> role based access
WEbhook Thirdparty autorization agent  "Open Policy Agent"  Kuibube AI makes api call to Open policy agent to determine if they are authorized

Authorization Modes:
NODE
ABAC
RBAC
WEbhook
alwaysallow
alwaysdeny


The mode is set on kube-apiserver.   If you do not set this it defaults to AlwaysAllow
--authorization-mode=

When you have multiple modes configure.  They are autoized by each one in the order specified. forwards to next one in list
if authorization fails


RBAC:

Create a role by creating a role object.

######developer-role.yaml#######
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: developer
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["list", "get", "create", "updated", "delete"]
- apiGroups: [""]
  resources: ["ConfigMap"]
  verbs: ["create"]
##############################
kubectl create -f developer-role.yaml

Link user to role : Create another object called a roll binding

########devuser-developer-binding.yaml##############
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: devuser-developer-binding
subjects:
- kind: User
  name: dev-user
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: developer
  apiGroup: rbac.authorization.k8s.io
####################################################
kubectl create -f devuser-developer-binding.yaml

  View RBAC: kubectl get roles
  View role bindings: kubectl get rolebindings
  kubectl describe role developer
  kubectl describe rolebinding devuser-developer-binding

Check you access in a cluster

kubectl auth can-i create deployments
kubectl auth can-i delete nodes
kubectl auth can-i create deployments --as dev-user
kubectl auth can-i create pods --as dev-user
kubectl auth can-i create pods --as dev-user --namespace test

you can restrict access to certain pods:

#################developer-role.yaml#####################
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: developer
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "create", "update"]
  resourceNames: ["blue". "orange"]
######################################################
kubectl create -f developer-role.yaml

Identify the authoriztion modes configured on cluster:
cat /etc/kubernetes/manifesrt/kube-apiserver.yaml   --> look at authorization mode.
ps -aux | grep authorization -> look at the kube-apiserver runtime options


k get pods --as dev-user


create: role:
kubectl create role developer --verb=list,create,delete --resource=pod

create rolebinding:
kubectl create rolebinding dev-user-binding --role=developer --user=dev-user

edit roles:

kubectl edit role developer -n blue


Namespace resources:
kubectl api-resources -namespaced=true

Non Namespaced resources:
kubectl api-resources --namespaced=false


Cluster roles and cluster role bindings is how you give user access to Cluster scoped resources:

####cluster-admin-role.yaml############
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoles
metadata:
  name: cluster-administrator
rules:
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["list", "get", "create", "delete"]
##########################################
kubectl create -f cluster-admin-role.yaml

#####cluster-admin-role-binding.yaml#######
apiVersion: rbac.authorization.k8s.io/v1
kind: clusterrolebinding
metadata:
  name: cluster-admin-role-binding
subjects:
- kind: User
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: Cluster-administrator
  apiGroup: rbac.authorization.k8s.io
#######################################
kubectl create -f cluster-admin-role-binding.yaml


k get clusterroles --no-headers | wc -l
k get clusterrolebindings --no-headers | wc -l


k create clusterrole michelle-role --verb=get,list,watch --resource=nodes
k create clusterrolebinding michelle-role-binding --clusterrole=michelle-role --user=michelle

kubectl api-resources -> list all the resources

k create clusterrole storage-admin --resource=persistentvolumes,storageclasses --verb=list,create,get,watch

controlplane ~ ✖ k --as michelle get storageclass


#####Service Accounts#######
kubectl create serviceaccount dashboard-sa -> create
kubectl get serviceaccount -> list all service Accounts

kubectl describe serviceaccount dashboard-sa  -> when serviceaccount is created it creats a token.  token is stored in secret object.

kubectl describe secret dashboard-sa-token-kbbdm -> to view the serviceaccount token

curl htttps://192.168.56.70:6443/api -insecure --header "Authorization: Bearer hjkbhnbjkghjbhj...."  -> use token to talk to api.


If serviceaccount is used by thirdparty application running in a pod inside cluster you would mount the token.

- For every namespace a service account called default exists.

- when a pod is created the default serviceacount token is automaticaly mounted to pod.

- in pod definition you can add "serviceAccountName:wood" to spec secion to mount a different serviceaccount
- if you do not want to mount any sewrvice account add "automountServiceAccountToken: false" to pod definition


create service account token:
kubectl create token dashboard-sa

kubectl get deployment web-dashboard -o yaml > dashboard.yaml
add "serviceAccountName: dashboard-sa" to pod spec of deployment.


###Image Security##########

- by default images are grabbed from default registry docker.io
- also if no user is given it will grabe it from default "library"

image: dosker.io/library/nginx

Google registry gcr.io/

To use private registry creds for image download:
- create secret with creds
kubectl create secret docker-registry regcred \
--docker-server=  private-registry.io  \
--docker-username= registry-user  \
--docker-password= registry-password  \
--docker-email=  registry-user@org.com  \

docker-registry is a built in secret type used for storing registry creds.

Pod spec section add :
imagePullSecrets:
- name: regcred

Docker Security

docker run --user=1001 ubuntu sleep 3600
docker run --cap-add MAC_ADMIN UBUNTU  -> full list of capabilities @ /usr/include/linux/capability.h

Security contexts:

adding security context to pod

apiVersion: v1
kind: Pod
metadata:
  name: web-pod
spec:
 securityContext:
   runAsUser: 10000

 containers:
      - name: ubuntu
        image: ubuntu
        command: ["sleep", "3690"]


-> if you was conteext at the container move it under containers section.

kubectl exec ubuntu-sleeper -- whoami  -> what using is running a pod.


to add capability add to container in pod definition
securityContext:
  capabilities:
    add: ["SYS_TIME"]


Network Policies:  firewall for pods

Network Policy - rules


#########policy-definition.yaml#########
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name:  db-policy
spec: 
  podSelector:
    matchLabels:
      role: db
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          name: api-pod
      namespaceSelector:   -> limit to a pod in a particular namespace   all "-" are considered or.  with out dash its treated as a AND
        matchLabels:
          name: prod 
    - ipBlock:
        cidr: 192.168.5.10/32 

    ports:
    - protocol: TCP
      port 3306
#######################################
kubectl create -f policy-definition.yaml


Support network policies:  Kube-router,Calico, Romana, Weave-network
Do not support Network policies: Flannel

Reference: https://github.com/ahmetb/kubectx



Kubectx:

With this tool, you don't have to make use of lengthy “kubectl config” commands to switch between contexts. This tool is particularly useful to switch context between clusters in a multi-cluster environment.



Installation:

sudo git clone https://github.com/ahmetb/kubectx /opt/kubectx
sudo ln -s /opt/kubectx/kubectx /usr/local/bin/kubectx


Syntax:

To list all contexts:

kubectx



To switch to a new context:

kubectx <context_name>



To switch back to previous context:

kubectx -



To see current context:

kubectx -c





Kubens:

This tool allows users to switch between namespaces quickly with a simple command.

Installation:

sudo git clone https://github.com/ahmetb/kubectx /opt/kubectx
sudo ln -s /opt/kubectx/kubens /usr/local/bin/kubens


Syntax:

To switch to a new namespace:

kubens <new_namespace>



To switch back to previous namespace:

kubens -


apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: internal-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      name: internal
  policyTypes:
    - Egress
  egress:
    - to:
        - podSelector:
            matchLabels:
              name: payroll
      ports:
        - protocol: TCP
          port: 8080

    - to:
      - podSelector:
          matchLabels:
            name: mysql
      ports:
        - protocol: TCP
          port: 3306


#################Docker Storage##############

Filesystenm  - default
/var/lib/docker/{aufs,containers,image,volumes}

Layered architecture

images use layered architecture.  each instruction in Dockerdile is another layer .
The advantage of layered archtecture is that it will share layers from other containers like images.  Does not start from scratch so builds faster 
and saves diskspace.


##Container layer gets created when running docker run##
Layer 6  Container Layer (writeable layer)  -> loose data when container stops .

###Image Layers###  dockerbuild (readonly)
Layer 5  Update Entryupoint with "flask" command
Layer 4  Source code
Layer 3  Changes in pip packages
Layer 2  Changes in apt packages
Layer 1  Base Ubuntu Layer


Creating persistent volumes:

- docker volume create data_volume
  creats /var/lib/docker/volumes/data_volume

- mount the volume to the container "docker run -v data_volume:/var/lib/mysql mysql"
- if volume does not exist docker will automatically create and mount to container when you run "docker run -v data_volume:/var/lib/mysql mysql"
- you can see all volumes "ls -lrt /var/lig/docker/volumes/"
- you can change container volume mountpoint(bind mounting) "docker run -v /data/mysql:/var/lib/mysql mysql
bind mount/volume mount

Storage drivers used by docker:
aufs
ZFS
BTRFS
Device Mapper
Overlay
Overlay2

Default storage driver depends on OS its running on.


###Adding volume to pod###
apiVersion: v1
kind: Pod
metadata:
  name: random-number-generator
spec:
  containers:
  - image: alpine
    name: alpine
    command: [ "/bin/sh", "-c" ]
    args: ["shuf -i 0-100 -n 1 >> /opt/numbers.out;"]
    volumeMounts:
    - mountPath: /opt
      name: data-volume

  volumes:
  - name: data-volume
    hostPath:   -> works on a single node but not multi host cluster
       path: /data
       type: Directory

using amazon elastic storage:
volumes:
- name: data-volume
  awsElasticBlockStore
    volumeID:  <volume-id.
    fstype: ext4



Kubernetes supported cluster solutions:
NFS
GlusterFS
Flocker
ceph
SCALEIO
etc..

####Persisent volumes kubernetes####

----pv-definition.yaml-----
apiVersion: v1
kind: persistentvolume
metadata:
  name: pv-vol1

spec:
  accessModes:
      - ReadWriteOnce
  capacity:
      storage: 1Gi


Persistent Volume Claims and Persistent Volumes are 2 seperate objects.

pvc-definition.yaml

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myclaim
spec:
  accessModes:
     - ReadWriteOnce
  resources:
     requests:
       storage: 500Mi

kubectl create -f pvc-definition.yaml


View claims:

kubectl get persistentvolumeclaim

kubectl delete persistentvolumeclaim myclaim

- when claimm in deleted it will retain the persistent volume by default. 
- you can change the behavior by adding "persistentVolumeReclaimPolicy: Delete" Recycle scrubs the data .

Using PVCs in Pods
Once you create a PVC use it in a POD definition file by specifying the PVC Claim name under persistentVolumeClaim section in the volumes section like this:



apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: myfrontend
      image: nginx
      volumeMounts:
      - mountPath: "/var/www/html"
        name: mypd
  volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: myclaim


The same is true for ReplicaSets or Deployments. Add this to the pod template section of a Deployment on ReplicaSet.



Reference URL: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#claims-as-volumes


kubectl exec webapp -- cat /log/app.log  -> view logs on pod

     - mountPath: /log
        name: log-volume

    - name: log-volume
      hostPath:
         path: /var/log/webapp
    

k replace --force -f webapp.yaml -> delete and recreat pod

access modes have to be the same in pv and pvc.

#### pv.yaml ####################
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-log
spec:
  capacity:
    storage: 100Mi
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  hostPath:
    path: /pv/log
#############################

#######pvc.yaml#########################
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: claim-log-1
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 50Mi
######################################
k replace --force -f pvc.yaml


Storage Classes:

Statis Provisioning:
1) create volume/disk in google cloud
2) create pv definition using the google cloud volume

 
Dynamic Provisioning - Storage classes are used

Create a Storage class definition: (only pvc definition is required.  sc creates the pv)
#########sc-definition.yaml#######################
apiVersion: storage.k8s.io/v1
kind: storageclass
metadata:
   name: google-storage
provisioner: kubernetes.io/gce-POD
##################################################

#########pvc-definition.yaml####################
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myclaim

spec:
  accessModes:
     - ReadWriteOnce
  storageClassName: google-storage
  resources:
     requests:
       storage: 500Mi
################################################


The Storage Class called local-storage makes use of VolumeBindingMode set to WaitForFirstConsumer. 
This will delay the binding and provisioning of a PersistentVolume until a Pod using the PersistentVolumeClaim is created.



apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: nginx
spec:
  containers:
  - image: nginx:alpine
    name: nginx
    resources: {}
    volumeMounts:
      - mountPath: "/var/www/html"
        name: local-pvc-volume
  dnsPolicy: ClusterFirst
  restartPolicy: Always
  volumes:
  - name: local-pvc-volume
    persistentVolumeClaim:
        claimName: locall-pvc
status: {}

Networking:

Switch routing:::
ip link  -> list interfaces
ip addr  -> see the ip on those interfaces
ip addr add 192.168.1.0/24 dev eth0   -> set ip on interfaces
ip route - see routing table
ip route add 192.168.1.0/24 via 192.168.2.1
cat /proc/sys/net/ipv4/ip_forward


DNS:

CoreDNS also supports other ways of configuring DNS entries through plugins. We will look at the plugin that it uses for Kubernetes in a later section.

Read more about CoreDNS here:

https://github.com/kubernetes/dns/blob/master/docs/specification.md

https://coredns.io/plugins/kubernetes/



#######Network namespaces###############

namespaces are the romms in house .  isolates 

To create a new network namespce on linux:
ip netns add red
ip netns add blue

List namespaces:
ip netns

To see links inside namespace:
ip netns exec red ip link
ip -n red link


arp table is isolated inside namespace:

ip netns exec red  arp


routing table is also isolated inside namespace:
ip netns exec red route

##############################################
On creattion network namespaces have not connectiviy.  setting up connectivity:

create a virtual link between the namespaces:
ip link add veth-red type veth peer name veth-blue

Next you have connect the link to interface:
ip link set veth-red netns red
ip link set venth-blue netns blue

Next you can attache ip to these ns interfaces and bring them online:
ip -n red addr add 192.168.15.1 dev veth-red
ip -n blue addr add 192.168.15.2 dev veth-blue
ip -n red link set veth-red up
ip -n blue link set veth-blue up

Now you can ping between namespaced     
ip netns red ping 192.168.15.7
if you look at arp table now you will see entries for 

When you multiple interfaces inside namespace you need to creat a virtual network network 
1) first create a Ns switch
on parent OS:
ip link add v-net-0 type bridge
ip link set dev v-net-0


creating link between network namespaces 

ip link add veth-red type veth peer name veth-blue
ip link set veth-red netns red
ip link set veth-blue netns blue
ip -n red addr add 192.168.15.1 dev veth-red
ip -n blue addr add 192.168.15.2 dev veth-blue

ip -n red link set veth-red up
ip -n blue link set veth-blue up

ip netns exec red ping 192.168.15.2

ip netns exec red arp
ip netns exec blue arp

If you have more then one namespaces:


####Linux Bridge#####
ip link add v-net-0 type bridge
ip link
ip link set v-net-0 up

Delete cable:  
ip -n red link del veth-red  -> when you delete one end the other gets deleted automatically

Commect cables to bridge:
# ip link add veth-red type veth peer name veth-red-br 
# ip link add veth-blue type veth peer name veth-blue-br

Connect network ns to bridge

ip link set veth-red netns red
ip link set veth-red-br master v-net-0


ip link set veth-blue netns blue
ip link set veth-blue-br master v-net-0 

assign ip:
ip -n red addr add 192.168.15.1 dev veth-red
ip -n blue addr add 192.168.15.2 dev veth-blue

bring up interface:

ip -n red link set veth-red up
ip -n blue link set veth-blue up

create a route so you can ping another host:
ip netns exec blue ip route add 192.168.1.0/24 via 192.168.15.5

you need nat for it to be able to talk to other host along with the route you created:
iptables -t nat -A POSTROUTING -s 192.168.15.0/24 -j MASQUERADE


Try to talk to internet from Ns
ip netns exec blue ip route add default via 192.168.15.5


Talk to app running on ns from outside:
iptables -t nat -A PREROUTING --dport 80 --to-destination 192.168.15.2:80 -j DNAT 


While testing the Network Namespaces, if you come across issues where you can't ping one namespace from the other, make sure you set the NETMASK while setting IP Address. ie: 192.168.1.10/24



ip -n red addr add 192.168.1.10/24 dev veth-red



Another thing to check is FirewallD/IP Table rules. Either add rules to IP Tables to allow traffic from one namespace to another. Or disable IP Tables all together (Only in a learning environment).


####Docker Networking#####

No networking:(nobody from outside world can reach it) no network
docker run --network none nginx

Host networking. use host networking:
docker run --network host nginx (if you run the same command again it will not work as port is already in use)


Bridge [internal private network is created that the docker containers connect to]

docker network ls

ip netns

ip -n jkljkljsdfs[container]  addr  -> container network

Docker has port mapping to allow ports to be accesses externally 
docker run -p 8080:80 nginx  -> behind the scenes it creates a dnat iptables rule.
You can see the nat rules:
iptables -nvL -t nat


CNI - Container Networking Interface: standards to write network plugins(scripts )

Plugin(scripts network) available:
Bridge
vlan
ipvlan
macvlan
windows
dhcp
host-local
Flannel
weaveworks



Docker does not implment CNI  has own set of standards.   CNM(Container Network Model)


Cluster Networking:
kublets on worker nodes listen on port 10250
services lissten on worker nodes port 30000-32767
kube-api on master listens on port 6443
kube-scheduler on master listens on port 10259
kube-controller on master listens on port 10257
etcd on master listens on 2379
if multi master etcd also has process listening on 2380

all this info found in kubernetes documentation


An important tip about deploying Network Addons in a Kubernetes cluster.



In the upcoming labs, we will work with Network Addons. This includes installing a network plugin in the cluster. While we have used weave-net as an example, please bear in mind that you can use any of the plugins which are described here:

https://kubernetes.io/docs/concepts/cluster-administration/addons/

https://kubernetes.io/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-networking-model



In the CKA exam, for a question that requires you to deploy a network addon, unless specifically directed, you may use any of the solutions described in the link above.



However, the documentation currently does not contain a direct reference to the exact command to be used to deploy a third-party network addon.

The links above redirect to third-party/vendor sites or GitHub repositories, which cannot be used in the exam. This has been intentionally done to keep the content in the Kubernetes documentation vendor-neutral.



NOTE: In the official exam, all essential CNI deployment details will be provided.


####What is the network interface configured for cluster connectivity on controlplane node(node-to-node)#########:
kubectl get nodes -o wide  -> get ip on controlplane node
ip addr -> look for what interface is using controlplane ip


#####Find bridge interfqces on a host##################
ip addr show type bridge


Correct! That's because 2379 is the port of ETCD to which all control plane components connect to. 2380 is only for etcd 
peer-to-peer connectivity. When you have multiple controlplane nodes. In this case we don't


################POD Networking####################
Networking Model:]
1) Every POD should have an IP Address
2) Every POD should be able to communicate with every other POD in the same node.
3) Every POD should be able to communicate with every other POD on other nodes without nat

CNI lets you run a network script everytime a pod is created(middleman):
CNI calls a sript in the correct format

CNI script:
##############net-script.sh############
ADD)
# Create veth particular
# Attach veth particular
# Assign IP Address
# Bring Up Interface
DEL)
# Delete veth pair
#####################################

- container Runtime on each node is responsible for cerating container
- Looks in cni conf directory
  /etc/cni/net.d/net-script.conflist
- Looks in cni bin drirectory to find binary
  /opt/cni/bin/net-script.sh

- runs script with options
  ./net-script.sh add <container> <namespace>


########Container Networking Interface(CNI) in kubernetes#########

Container runtime is responsible for the following:
- Container Runtime must create network namespace
- Identify network the container must attach to
- Container Runtime to invoke Network Plugin(bridge) when container is added
- Container Runtime to invoke Network Plugin(bridge) when cojntainer is Deleted.
- JSON format of the Network Configuration

Kubernetes container run times :  containerd and cri-object

All plugins are installed under /opt/cni/bin
Configs for plugins are under /etc/cni/net.d



container runtime looks in /etc/cni/net.d to determine what plugin it should use.  If their are multiple it will use the first one listed
in alphabetical order

Example 10-bridge.conf  -> formating is defined by CNI standards
{
    "cniVersion": "0.2.0",
    "name": "mysql"
    "type": "bridge",
    "bridge": "cni0",
    "isGateway": true,
    "ipMasq": true,
    "ipam": {
       "type": "host-local",
       "subnet": "10.22.0.0/16",
       "routes": [
          {  "dst": "0.0.0.0/0"}
       ]
    }


}


Important Update: -

Before going to the CNI weave lecture, we have an update for the Weave Net installation link. They have announced the end of service for Weave Cloud.

To know more about this, read the blog from the link below: -

https://www.weave.works/blog/weave-cloud-end-of-service

As an impact, the old weave net installation link won’t work anymore: -

kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"

Instead of that, use the below latest link to install the weave net: -

kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml

Reference links: -

https://www.weave.works/docs/net/latest/kubernetes/kube-addon/#-installation

https://github.com/weaveworks/weave/releases


###############################CNI Weave ####################################

- weave deploys a agent/service on each node and they communicate with one another 

- Deploy Weave on a kubernetes cluster:

  Weave agents are installed as a daemonset on all nodes  using kubctl
  kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"
   
- you can see the weave agents/daemon set pods from controller:
  kubectl get pods -n kube-system

- get the logs from weave pod:
  kubectl logs weave-net-5gcmb weave -n kube-system


- where to find container runtime of kublet
ps -ef | grep kublet   -> kublet process as a variable --container-runtime


- what binary executable file will be run by kubelet after a container and namespave is created:
lookin config  under "/etc/cni/net.d"  and type points to the executable.



########IPAM(IP address management) CNI###########
How is the bridge assigned a subnet and who detmines what ip is assigned to pods(no duplicate ip assigned)

= manage ip with a local file on yeach node
    ip = get_free_ip_from_host_local()

cni module config has section for ipam: "/etc/cni/net.d/net-script.conf
  "ipam": {
      "type": "host-local",
      "subnet": "10.244.0.0/16",
      "routes": [
         { "dst": "0.0.0.0/0" }
      ]
  }

  ####How Weaveworks assigns/manages ip addesses######
  By default weave uses subnet 10.32.0.0/12   (10.32.0.1 -> 10.47.255.254)
  The peers split the ips up and each peer has a different range

#### How many weave agents/peers are deployed in this cluster #####
k get pods -n kube-system  -> tells how many weave agents/peers are deployed in cluster


#### What is the POD IP address range configured by weave ? #############
kubectl logs -n kube-system weave-net-ftrt -> look at logs of a deplyed weave pod "ipalloc-range:10.244.0.0/16 "

#### What is the default gateway configureed on the PODs scheduled on node01 ######
- easiet way to do this is just deploy a pod to node01 and look what derfault gateway is:
1) kubectl run busybox --image=busybox --dry-run=client -o yaml -- sleep 1000 > busybox.yaml
2) edit busybox.yaml and add "nodeName: node01" to "spec:" section
3) kubectl apply -f busybox.yaml
4) kubectl exec busybox -- netstat -rn   -> get default route for pod deployed on node01



############################service networking ######################################


service: clusterIP
- service is used to make pod reachable feom other pod.
- service is cluster wide and can be used from any node
- service is only accessed from within cluster 


service: Node port
- can be accessed outside of cluster
- exposes application port on all nodes in cluster


* when new service is created kube-proxy gets into action and runs on every node. services are cluster wide
* when service is created it gets assigned a ip and kube-proxy sets up forwarding rules on all nodes in cluster
* kube-proxy can use userspace,iptables or ipvs to setup the forwarding rule for service ip:port
* you can set what kube-proxy uses for forwarding. It will default to iptables if nothing is defined
* the service ip used is determined by "kube-api-server --service-cluster-ip-range ipNet (default: 10.0.0.0/24)
* ps aux | grep kube-api-server -> look to findout what service-cluster-ip-rangee is set to.


* iptables -L -t nat -> see what iptable rules are put in place for service
* cat /var/log/kube-proxy.log  -> you can see what its doing for service

*****************What type of proxy is the kube-proxy configured to use?###################
kubectl -n kube-system logs kube-proxy-rq2vb  -> look n logs of one of the proxy pods to get the type of proxy being used

#################################Cluster DNS##############################################
* whenever a service is created kube dns maps the name to ip
* with in the cluster any pod can access the service using the name
* if you are trying to talk to a serveice in a differnt namespace you need to specify that "curl http://web-service.apps"
* curl curl htt://[Hostname].[namespace].[type].[root]
* example: cutl http://web-service.apps.svc.cluster.local
* names to ip are not automatically created for pods by default
* for a pod the ip address is used in hostanme with "." replaced by "-"
* example: curl http://10-244-2-5.apps.pod.cluster.local


##################################Ho kubernets implements DNS #####################
* use to be kube-dns but is core-dns now.
* /etc/coredns/Corefile  -> pod dns needs to be enabled with a entrypoint.  file is passed in to pod as a configmap
* kubectl get configmap -n kube-system
* when you dploy coredns to cluster and service is created also.
* kubectl get service -n kube-system  -> it will be called kube-dns
* pods add the coredns service ip to /etc/resolv.conf
* kublet takes care of setting up resolv.conf to core dns service





