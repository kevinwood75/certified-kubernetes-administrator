git clone https://github.com/kevinwood75/certified-kubernetes-administrator-course.git
cd /Users/kwood/projects/coding-repos/certified-kubernetes-administrator-course/kubeadm-clusters/virtualbox

https://developer.hashicorp.com/vagrant/docs/cli/destroy

vagrant up   -> build out vm as per vagrant file.
vagreant status -> status of vm
vagrant halt -> stops all vm specified in vagrant file "you use up to atart back up"
vagrant destory -> removes the vm's 
vagrant ssh -> connect to created vm


	
Vagrant Cloud is Moving
Vagrant Cloud is moving to the HashiCorp Cloud Platform (HCP) as the Vagrant Box Registry. If you are a new user, please create an HCP account and continue to the HCP Vagrant Box Registry.

Existing Vagrant Cloud users can still login below to manage or migrate their boxes.



https://portal.cloud.hashicorp.com/services/vagrant/registries


vagrant box registry:  https://portal.cloud.hashicorp.com/vagrant/discover?architectures=arm64&providers=virtualbox&query=ubuntu


VBoxManage: error: Details: code NS_ERROR_FAILURE (0x80004005), component SessionMachine, interface ISession

Multipass:
https://canonical.com/multipass


LAB:
192.168.2.213 - loadbalancer
192.168.2.111  - controlplane
192.168.2.110 - controlplane02
192.168.2.112 - node01
192.168.2.113 - node02


https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/


https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/

https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/

https://kubernetes.io/docs/setup/production-environment/container-runtimes/#cgroup-drivers

https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/configure-cgroup-driver/


https://kubernetes.io/docs/concepts/cluster-administration/addons/
################Installing kubernetes cluster#####################

1) sudo mkdir -p -m 755 /etc/apt/keyrings
2) curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.32/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

3) echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.32/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list

echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list

4)  
sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
***to install specific minor version
apt install -y kubeadm=1.31.0-1.1 kubelet=1.31.0-1.1 kubectl=1.31.0-1.1

****get version installed
node01 ~ âœ– kubelet --version
Kubernetes v1.31.0

sudo apt-mark hold kubelet kubeadm kubectl

##################Creating a K8s cluster with kubeadm
******on all nodes do the following
root@node01:/# tee /etc/modules-load.d/containerd.conf <<EOF
overlay
br_netfilter
EOF

modprobe br_netfilter
modprobe overlay

tee /etc/sysctl.d/kubernetes.conf<<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF

sysctl --system

****swapoff -a && sudo sed -i '/swap/d' /etc/fstab
*****rm -f  swap.img
********************************************************

1) install container runtime
apt update
apt install -y containerd
** 2 cgroup drivers are available.  cgroupfs and systemd.  Both container run time and kubelet need to set to same cgroup driver
*** The cgroupfs driver is not recommended when systemd is the init system because systemd expects a single cgroup manager on the system. Additionally, if you use cgroup v2, use the systemd cgroup driver instead of cgroupfs
2) verify if systemd is used ->  ps -p 1
****In v1.22 and later, if the user does not set the cgroupDriver field under KubeletConfiguration, kubeadm defaults it to systemd.
3) mkdir -p /etc/containerd
4) spit out default containerd config ->  containerd config default
5) create containd config:
containerd config default | sed 's/SystemdCgroup = false/SystemdCgroup = true/' | tee /etc/containerd/config.toml
6) restart containerd after config change -> systemctl restart containerd
7) look at containerd config being used  ->  containerd config dump | grep -i systemdcgroup
8) initialize on controlplane node:
swapoff -a && sudo sed -i '/swap/d' /etc/fstab
kubeadm init --apiserver-advertise-address 192.168.2.111 --pod-network-cidr "10.244.0.0/16" --upload-certs 


kubeadm init --apiserver-advertise-address 192.18.84.12 --apiserver-cert-extra-sans controlplane --pod-network-cidr "10.244.0.0/16" --upload-certs 

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.2.111:6443 --token 4f72x5.mysjrrmatnevnzzf \
	--discovery-token-ca-cert-hash sha256:4446e41896e0853dd8f0189c75412c3d610e390cd872f9e3064d1d06bcf59c0b 


9) create pod network
   * decided on flannel

   ****If you use custom podCIDR (not 10.244.0.0/16) you first need to download the above manifest and modify the network to match your one.
   https://github.com/flannel-io/flannel#deploying-flannel-manually
   kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml

******* to specify what interface to user change kube-flannel.yml before installing
I've the same problem, trying to use k8s and Vagrant. I've found this note in the documentation of flannel:

Vagrant typically assigns two interfaces to all VMs. The first, for which all hosts are assigned the IP address 10.0.2.15, is for external traffic that gets NATed.

This may lead to problems with flannel. By default, flannel selects the first interface on a host. This leads to all hosts thinking they have the same public IP address. To prevent this issue, pass the --iface eth1 flag to flannel so that the second interface is chosen.

So I look for it in the flannel's pod configuration. If you download the kube-flannel.yml file, you should look at DaemonSet spec, specifically at the "kube-flannel" container. There, you should add the required "--iface=enp0s8" argument (Don't forget the "="). Part of the code I've used.

  containers:
  - name: kube-flannel
    image: quay.io/coreos/flannel:v0.10.0-amd64
    command:
    - /opt/bin/flanneld
    args:
    - --ip-masq
    - --kube-subnet-mgr
    - --iface=enp0s8

https://github.com/flannel-io/flannel#deploying-flannel-manually

10) join workers to controlplane
kubeadm join 192.168.2.111:6443 --token 4f72x5.mysjrrmatnevnzzf \
	--discovery-token-ca-cert-hash sha256:4446e41896e0853dd8f0189c75412c3d610e390cd872f9e3064d1d06bcf59c0b




  ##################################Building out Woodez K8s Cluster#####################################

  LAB infra:
   192.168.2.213 - loadbalancer
   192.168.2.111  - controlplane
   192.168.2.110 - controlplane02
   192.168.2.109 - controlplane03
   192.168.2.112 - node01
   192.168.2.113 - node02

https://github.com/kubernetes/kubeadm/blob/main/docs/ha-considerations.md#options-for-software-load-balancing


-------------------------------Load balancer setup----------------------------

1) haproxy install
apt get update
apt get install haproxy

2) #########/etc/haproxy/haproxy.cfg##################

global
	log /dev/log	local0
	log /dev/log	local1 notice
	chroot /var/lib/haproxy
	stats socket /run/haproxy/admin.sock mode 660 level admin
	stats timeout 30s
	user haproxy
	group haproxy
	daemon

defaults
  mode                    http
  log                     global
  option                  httplog
  option                  dontlognull
  option http-server-close
  option forwardfor       except 127.0.0.0/8
  option                  redispatch
  retries                 1
  timeout http-request    10s
  timeout queue           20s
  timeout connect         5s
  timeout client          35s
  timeout server          35s
  timeout http-keep-alive 10s
  timeout check           10s

# apiserver frontend which proxies to control plane nodes

frontend apiserver
  bind *:6443
  mode tcp
  option tcplog
  default_backend apiserverbackend

# round robin balancing for apiserver

backend apiserverbackend
  option httpchk

  http-check connect ssl
  http-check send meth GET uri /healthz
  http-check expect status 200

  mode tcp
  balance roundrobin

  server controlplane01 192.168.2.111:6443 check verify none
  server controlplane02 192.168.2.110:6443 check verify none

3) systemctl restart haproxy


**************To remove k8s do on all nodes*****************
sudo kubeadm reset   
sudo apt-get remove kubeadm kubelet kubectl
************************************************************

4) installing kubeadm kubelet kubectl on all nodes
   sudo mkdir -p -m 755 /etc/apt/keyrings
   curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
   sudo rm -f /etc/apt/sources.list.d/kubernetes.list
   echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list
   sudo apt-get update
   sudo apt-get install -y kubelet kubeadm kubectl
   sudo apt-mark hold kubelet kubeadm kubectl
   sudo systemctl enable --now kubelet
   
   ******on all nodes do the following
   sudo tee /etc/modules-load.d/containerd.conf <<EOF
   overlay
   br_netfilter
   EOF

   sudo modprobe br_netfilter
   sudo modprobe overlay

   sudo tee /etc/sysctl.d/kubernetes.conf<<EOF
   net.bridge.bridge-nf-call-ip6tables = 1
   net.bridge.bridge-nf-call-iptables = 1
   net.ipv4.ip_forward = 1
   EOF

   sudo sysctl --system

   sudo swapoff -a && sudo sed -i '/swap/d' /etc/fstab
   sudo rm -f /swap.img

5) install container runtime
   sudo apt update
   sudo apt install -y containerd
   ** 2 cgroup drivers are available.  cgroupfs and systemd.  Both container run time and kubelet need to set to same cgroup driver
   *** The cgroupfs driver is not recommended when systemd is the init system because systemd expects a single cgroup manager on the system. Additionally, if you use cgroup v2, use the systemd cgroup driver instead of cgroupfs
   **** verify if systemd is used ->  ps -p 1
   **** In v1.22 and later, if the user does not set the cgroupDriver field under KubeletConfiguration, kubeadm defaults it to systemd
   sudo mkdir -p /etc/containerd
   **** spit out default containerd config ->  containerd config default
   **** create containd config:
   sudo containerd config default | sed 's/SystemdCgroup = false/SystemdCgroup = true/' | sudo tee /etc/containerd/config.toml
   sudo systemctl restart containerd
   sudo containerd config dump | grep -i systemdcgroup

6) initialize on controlplane node 
sudo kubeadm init --control-plane-endpoint "192.168.2.213:6443" --apiserver-cert-extra-sans controlplane --pod-network-cidr "10.244.0.0/16" --upload-certs

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of the control-plane node running the following command on each as root:

  sudo kubeadm join 192.168.2.213:6443 --token 2l36sg.h3mpxhplgzngdwoi \
	--discovery-token-ca-cert-hash sha256:c86917e89701dc7c202764131d737412c339604999435c116667a8f3eb46a009 \
	--control-plane --certificate-key 0465e13cefb6e34182875b57581a2e2e4a2d25576fb642aa10b70c0ab4376720

Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use
"kubeadm init phase upload-certs --upload-certs" to reload certs afterward.

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.2.213:6443 --token 2l36sg.h3mpxhplgzngdwoi \
	--discovery-token-ca-cert-hash sha256:c86917e89701dc7c202764131d737412c339604999435c116667a8f3eb46a009 

7) join second controlplane
sudo kubeadm join 192.168.2.213:6443 --token 2l36sg.h3mpxhplgzngdwoi \
	--discovery-token-ca-cert-hash sha256:c86917e89701dc7c202764131d737412c339604999435c116667a8f3eb46a009 \
	--control-plane --certificate-key 0465e13cefb6e34182875b57581a2e2e4a2d25576fb642aa10b70c0ab4376720

This node has joined the cluster and a new control plane instance was created:

* Certificate signing request was sent to apiserver and approval was received.
* The Kubelet was informed of the new secure connection details.
* Control plane label and taint were applied to the new node.
* The Kubernetes control plane instances scaled up.
* A new etcd member was added to the local/stacked etcd cluster.

To start administering your cluster from this node, you need to run the following as a regular user:

	mkdir -p $HOME/.kube
	sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
	sudo chown $(id -u):$(id -g) $HOME/.kube/config

Run 'kubectl get nodes' to see this node join the cluster.

8) join worker nodes
sudo kubeadm join 192.168.2.213:6443 --token 2l36sg.h3mpxhplgzngdwoi \
	--discovery-token-ca-cert-hash sha256:c86917e89701dc7c202764131d737412c339604999435c116667a8f3eb46a009

9) create pod network

run on control plane
kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml

10) cluster working :)

#########################K8s etcd####################################
https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/
https://etcd.io/docs/v3.6/op-guide/recovery/#restoring-a-cluster
https://etcd.io/docs/v3.5/op-guide/maintenance/

***ETCD member list from etcd container
kubectl exec etcd-controlplane -n kube-system -- sh -c "ETCDCTL_API=3 etcdctl --endpoints=127.0.0.1:2379 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key member list"
585d80bb4063b7b5, started, controlplane, https://192.168.2.111:2380, https://192.168.2.111:2379, false
78b3ef7922ed2c96, started, controlplane02, https://192.168.2.110:2380, https://192.168.2.110:2379, false

***ETCD member status from etcd container
 kubectl exec etcd-controlplane -n kube-system -- sh -c "ETCDCTL_API=3 etcdctl --cluster=true --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key --write-out=table endpoint status"
+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
|          ENDPOINT          |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |
+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
| https://192.168.2.111:2379 | 585d80bb4063b7b5 |  3.5.15 |  2.1 MB |      true |      false |         2 |     140771 |             140771 |        |
| https://192.168.2.110:2379 | 78b3ef7922ed2c96 |  3.5.15 |  2.1 MB |     false |      false |         2 |     140771 |             140771 |        |
+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+

***ETCD cluster health from etcd container
kubectl exec etcd-controlplane -n kube-system -- sh -c "ETCDCTL_API=3 etcdctl --cluster=true --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key --write-out=table endpoint health"
+----------------------------+--------+------------+-------+
|          ENDPOINT          | HEALTH |    TOOK    | ERROR |
+----------------------------+--------+------------+-------+
| https://192.168.2.111:2379 |   true | 9.022405ms |       |
| https://192.168.2.110:2379 |   true | 9.146941ms |       |
+----------------------------+--------+------------+-------+

***ETCD cluster hash status from etcd container
kubectl exec etcd-controlplane -n kube-system -- sh -c "ETCDCTL_API=3 etcdctl --cluster=true --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key --write-out=table endpoint hashkv"
+----------------------------+-----------+
|          ENDPOINT          |   HASH    |
+----------------------------+-----------+
| https://192.168.2.111:2379 | 647937148 |
| https://192.168.2.110:2379 | 647937148 |
+----------------------------+-----------+

*** run from client outside of etcd container

ETCDCTL_API=3 sudo etcdctl --endpoints=192.168.2.111:2379 --cluster=true --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key --write-out=table endpoint hashkv 
+----------------------------+------------+
|          ENDPOINT          |    HASH    |
+----------------------------+------------+
| https://192.168.2.111:2379 | 3750463371 |
| https://192.168.2.110:2379 | 3750463371 |
+----------------------------+------------+

*** Taking etcd cluster snapshot.
ETCDCTL_API=3 sudo etcdctl --endpoints=192.168.2.111:2379 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key snapshot save /tmp/snapshot-01062025.db
{"level":"info","ts":1736191163.313979,"caller":"snapshot/v3_snapshot.go:119","msg":"created temporary db file","path":"/tmp/snapshot-01062025.db.part"}
{"level":"info","ts":"2025-01-06T19:19:23.32106Z","caller":"clientv3/maintenance.go:212","msg":"opened snapshot stream; downloading"}
{"level":"info","ts":1736191163.3212695,"caller":"snapshot/v3_snapshot.go:127","msg":"fetching snapshot","endpoint":"192.168.2.111:2379"}
{"level":"info","ts":"2025-01-06T19:19:23.33989Z","caller":"clientv3/maintenance.go:220","msg":"completed snapshot read; closing"}
{"level":"info","ts":1736191163.3493383,"caller":"snapshot/v3_snapshot.go:142","msg":"fetched snapshot","endpoint":"192.168.2.111:2379","size":"2.1 MB","took":0.035198544}
{"level":"info","ts":1736191163.3494608,"caller":"snapshot/v3_snapshot.go:152","msg":"saved","path":"/tmp/snapshot-01062025.db"}
Snapshot saved at /tmp/snapshot-01062025.db

*** Checking snapshot status
ETCDCTL_API=3 sudo etcdctl --endpoints=192.168.2.111:2379 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key snapshot --write-out=table status /tmp/snapshot-01062025.db
+----------+----------+------------+------------+
|   HASH   | REVISION | TOTAL KEYS | TOTAL SIZE |
+----------+----------+------------+------------+
| 7a7a5d68 |   127606 |        857 |     2.1 MB |
+----------+----------+------------+------------+


**** adding another controler plane to cluster***************************
sudo kubeadm join 192.168.2.213:6443 --token 2l36sg.h3mpxhplgzngdwoi \
	--discovery-token-ca-cert-hash sha256:c86917e89701dc7c202764131d737412c339604999435c116667a8f3eb46a009 \
	--control-plane --certificate-key 0465e13cefb6e34182875b57581a2e2e4a2d25576fb642aa10b70c0ab4376720

join new controller to cluster after installing and setting up.  you may need to regenerate key depending how much time has passed

*************************************************************************


##########setting up k8s external loadbalancer#################
create a nginx loadbalancer:
https://cloudinfrastructureservices.co.uk/nginx-load-balancing/

apt-get install nginx -y
systemctl start nginx 
systemctl enable nginx
################################/etc/nginx/conf.d/loadbalancer.conf#################################
upstream backend {
	server 192.168.2.112:30008;
	server 192.168.2.113:30008;
   }

   server {
       listen	80;
       server_name 192.168.2.213;


       location / {
	   proxy_redirect	off;
	   proxy_set_header	X-Real-IP $remote_addr;
	   proxy_set_header	X-Forward-For $proxy_add_x_forwarded_for;
	   proxy_set_header	Host $http_host;
       proxy_pass http://backend;
    }

}  
#####################################################################################################



############setting up nginx ingress controller#########################






