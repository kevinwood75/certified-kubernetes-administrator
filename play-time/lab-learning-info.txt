git clone https://github.com/kevinwood75/certified-kubernetes-administrator-course.git
cd /Users/kwood/projects/coding-repos/certified-kubernetes-administrator-course/kubeadm-clusters/virtualbox

https://developer.hashicorp.com/vagrant/docs/cli/destroy

vagrant up   -> build out vm as per vagrant file.
vagreant status -> status of vm
vagrant halt -> stops all vm specified in vagrant file "you use up to atart back up"
vagrant destory -> removes the vm's 
vagrant ssh -> connect to created vm


	
Vagrant Cloud is Moving
Vagrant Cloud is moving to the HashiCorp Cloud Platform (HCP) as the Vagrant Box Registry. If you are a new user, please create an HCP account and continue to the HCP Vagrant Box Registry.

Existing Vagrant Cloud users can still login below to manage or migrate their boxes.



https://portal.cloud.hashicorp.com/services/vagrant/registries


vagrant box registry:  https://portal.cloud.hashicorp.com/vagrant/discover?architectures=arm64&providers=virtualbox&query=ubuntu


VBoxManage: error: Details: code NS_ERROR_FAILURE (0x80004005), component SessionMachine, interface ISession

Multipass:
https://canonical.com/multipass


LAB:
192.168.2.213 - loadbalancer
192.168.2.111  - controlplane
192.168.2.110 - controlplane02
192.168.2.112 - node01
192.168.2.113 - node02


https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/


https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/

https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/

https://kubernetes.io/docs/setup/production-environment/container-runtimes/#cgroup-drivers

https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/configure-cgroup-driver/


https://kubernetes.io/docs/concepts/cluster-administration/addons/
################Installing kubernetes cluster#####################

1) sudo mkdir -p -m 755 /etc/apt/keyrings
2) curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.32/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

3) echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.32/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list

echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list

4)  
sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
***to install specific minor version
apt install -y kubeadm=1.31.0-1.1 kubelet=1.31.0-1.1 kubectl=1.31.0-1.1

****get version installed
node01 ~ âœ– kubelet --version
Kubernetes v1.31.0

sudo apt-mark hold kubelet kubeadm kubectl

##################Creating a K8s cluster with kubeadm
******on all nodes do the following
root@node01:/# tee /etc/modules-load.d/containerd.conf <<EOF
overlay
br_netfilter
EOF

modprobe br_netfilter
modprobe overlay

tee /etc/sysctl.d/kubernetes.conf<<EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF

sysctl --system

****swapoff -a && sudo sed -i '/swap/d' /etc/fstab
*****rm -f  swap.img
********************************************************

1) install container runtime
apt update
apt install -y containerd
** 2 cgroup drivers are available.  cgroupfs and systemd.  Both container run time and kubelet need to set to same cgroup driver
*** The cgroupfs driver is not recommended when systemd is the init system because systemd expects a single cgroup manager on the system. Additionally, if you use cgroup v2, use the systemd cgroup driver instead of cgroupfs
2) verify if systemd is used ->  ps -p 1
****In v1.22 and later, if the user does not set the cgroupDriver field under KubeletConfiguration, kubeadm defaults it to systemd.
3) mkdir -p /etc/containerd
4) spit out default containerd config ->  containerd config default
5) create containd config:
containerd config default | sed 's/SystemdCgroup = false/SystemdCgroup = true/' | tee /etc/containerd/config.toml
6) restart containerd after config change -> systemctl restart containerd
7) look at containerd config being used  ->  containerd config dump | grep -i systemdcgroup
8) initialize on controlplane node:
swapoff -a && sudo sed -i '/swap/d' /etc/fstab
kubeadm init --apiserver-advertise-address 192.168.2.111 --pod-network-cidr "10.244.0.0/16" --upload-certs 


kubeadm init --apiserver-advertise-address 192.18.84.12 --apiserver-cert-extra-sans controlplane --pod-network-cidr "10.244.0.0/16" --upload-certs 

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.2.111:6443 --token 4f72x5.mysjrrmatnevnzzf \
	--discovery-token-ca-cert-hash sha256:4446e41896e0853dd8f0189c75412c3d610e390cd872f9e3064d1d06bcf59c0b 


9) create pod network
   * decided on flannel

   ****If you use custom podCIDR (not 10.244.0.0/16) you first need to download the above manifest and modify the network to match your one.
   https://github.com/flannel-io/flannel#deploying-flannel-manually
   kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml

******* to specify what interface to user change kube-flannel.yml before installing
I've the same problem, trying to use k8s and Vagrant. I've found this note in the documentation of flannel:

Vagrant typically assigns two interfaces to all VMs. The first, for which all hosts are assigned the IP address 10.0.2.15, is for external traffic that gets NATed.

This may lead to problems with flannel. By default, flannel selects the first interface on a host. This leads to all hosts thinking they have the same public IP address. To prevent this issue, pass the --iface eth1 flag to flannel so that the second interface is chosen.

So I look for it in the flannel's pod configuration. If you download the kube-flannel.yml file, you should look at DaemonSet spec, specifically at the "kube-flannel" container. There, you should add the required "--iface=enp0s8" argument (Don't forget the "="). Part of the code I've used.

  containers:
  - name: kube-flannel
    image: quay.io/coreos/flannel:v0.10.0-amd64
    command:
    - /opt/bin/flanneld
    args:
    - --ip-masq
    - --kube-subnet-mgr
    - --iface=enp0s8

https://github.com/flannel-io/flannel#deploying-flannel-manually

10) join workers to controlplane
kubeadm join 192.168.2.111:6443 --token 4f72x5.mysjrrmatnevnzzf \
	--discovery-token-ca-cert-hash sha256:4446e41896e0853dd8f0189c75412c3d610e390cd872f9e3064d1d06bcf59c0b




  ##################################Building out Woodez K8s Cluster#####################################

NTPD:
sudo apt update
sudo apt install ntp
sudo vi /etc/ntp.conf
server 0.ubuntu.pool.ntp.org
server 1.ubuntu.pool.ntp.org
server 2.ubuntu.pool.ntp.org
server 3.ubuntu.pool.ntp.org
sudo systemctl restart ntp
sudo systemctl status ntp
TZ='America/Toronto'; export TZ






  LAB infra:
   192.168.2.213 - loadbalancer xx
   192.168.2.111  - controlplane xx
   192.168.2.110 - controlplane02 xx
   192.168.2.114 - controlplane03 xx
   192.168.2.112 - node01 xx 
   192.168.2.113 - node02 xx x -> decommed
   192.168.2.107 - node02-woodez -> decommed
   192.168.2.108 - node03 xx
   192.168.2.109 - node04
   192.168.2.115 - node05


https://linuxconfig.org/setting-a-static-ip-address-in-ubuntu-24-04-via-the-command-line
https://github.com/kubernetes/kubeadm/blob/main/docs/ha-considerations.md#options-for-software-load-balancing

Increase diskspace:
lvresize -L +8G /dev/ubuntu-vg/ubuntu-lv
resize2fs /dev/ubuntu-vg/ubuntu-lv


-------------------------------Load balancer setup----------------------------

1) haproxy install
apt get update
apt get install haproxy

2) #########/etc/haproxy/haproxy.cfg##################

global
	log /dev/log	local0
	log /dev/log	local1 notice
	chroot /var/lib/haproxy
	stats socket /run/haproxy/admin.sock mode 660 level admin
	stats timeout 30s
	user haproxy
	group haproxy
	daemon

defaults
  mode                    http
  log                     global
  option                  httplog
  option                  dontlognull
  option http-server-close
  option forwardfor       except 127.0.0.0/8
  option                  redispatch
  retries                 1
  timeout http-request    10s
  timeout queue           20s
  timeout connect         5s
  timeout client          35s
  timeout server          35s
  timeout http-keep-alive 10s
  timeout check           10s

# apiserver frontend which proxies to control plane nodes

frontend apiserver
  bind *:6443
  mode tcp
  option tcplog
  default_backend apiserverbackend

# round robin balancing for apiserver

backend apiserverbackend
  option httpchk

  http-check connect ssl
  http-check send meth GET uri /healthz
  http-check expect status 200

  mode tcp
  balance roundrobin

  server controlplane01 192.168.2.111:6443 check verify none
  server controlplane02 192.168.2.110:6443 check verify none

3) systemctl restart haproxy


**************To remove k8s do on all nodes*****************
sudo kubeadm reset   
sudo apt-get remove kubeadm kubelet kubectl
************************************************************

4) installing kubeadm kubelet kubectl on all nodes
   sudo mkdir -p -m 755 /etc/apt/keyrings
   curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.31/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
   sudo rm -f /etc/apt/sources.list.d/kubernetes.list
   echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.31/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list
   sudo apt-get update
   sudo apt-get install -y kubelet kubeadm kubectl
   sudo apt-mark hold kubelet kubeadm kubectl
   sudo systemctl enable --now kubelet
   
   ******on all nodes do the following
   sudo tee /etc/modules-load.d/containerd.conf <<EOF
   overlay
   br_netfilter
   EOF

   sudo modprobe br_netfilter
   sudo modprobe overlay

   sudo tee /etc/sysctl.d/kubernetes.conf<<EOF
   net.bridge.bridge-nf-call-ip6tables = 1
   net.bridge.bridge-nf-call-iptables = 1
   net.ipv4.ip_forward = 1
   EOF

   sudo sysctl --system

   sudo swapoff -a && sudo sed -i '/swap/d' /etc/fstab
   sudo rm -f /swap.img

5) install container runtime
   sudo apt update
   sudo apt install -y containerd
   ** 2 cgroup drivers are available.  cgroupfs and systemd.  Both container run time and kubelet need to set to same cgroup driver
   *** The cgroupfs driver is not recommended when systemd is the init system because systemd expects a single cgroup manager on the system. Additionally, if you use cgroup v2, use the systemd cgroup driver instead of cgroupfs
   **** verify if systemd is used ->  ps -p 1
   **** In v1.22 and later, if the user does not set the cgroupDriver field under KubeletConfiguration, kubeadm defaults it to systemd
   sudo mkdir -p /etc/containerd
   **** spit out default containerd config ->  containerd config default
   **** create containd config:
   sudo containerd config default | sed 's/SystemdCgroup = false/SystemdCgroup = true/' | sudo tee /etc/containerd/config.toml
   sudo systemctl restart containerd
   sudo containerd config dump | grep -i systemdcgroup

6) initialize on controlplane node 
sudo kubeadm init --control-plane-endpoint "192.168.2.213:6443" --apiserver-cert-extra-sans controlplane --pod-network-cidr "10.244.0.0/16" --upload-certs

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of the control-plane node running the following command on each as root:

  sudo kubeadm join 192.168.2.213:6443 --token 2l36sg.h3mpxhplgzngdwoi \
	--discovery-token-ca-cert-hash sha256:c86917e89701dc7c202764131d737412c339604999435c116667a8f3eb46a009 \
	--control-plane --certificate-key 0465e13cefb6e34182875b57581a2e2e4a2d25576fb642aa10b70c0ab4376720

Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use
"kubeadm init phase upload-certs --upload-certs" to reload certs afterward.

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.2.213:6443 --token 2l36sg.h3mpxhplgzngdwoi \
	--discovery-token-ca-cert-hash sha256:c86917e89701dc7c202764131d737412c339604999435c116667a8f3eb46a009 

7) join second controlplane
sudo kubeadm join 192.168.2.213:6443 --token 2l36sg.h3mpxhplgzngdwoi \
	--discovery-token-ca-cert-hash sha256:c86917e89701dc7c202764131d737412c339604999435c116667a8f3eb46a009 \
	--control-plane --certificate-key 0465e13cefb6e34182875b57581a2e2e4a2d25576fb642aa10b70c0ab4376720

This node has joined the cluster and a new control plane instance was created:

* Certificate signing request was sent to apiserver and approval was received.
* The Kubelet was informed of the new secure connection details.
* Control plane label and taint were applied to the new node.
* The Kubernetes control plane instances scaled up.
* A new etcd member was added to the local/stacked etcd cluster.

To start administering your cluster from this node, you need to run the following as a regular user:

	mkdir -p $HOME/.kube
	sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
	sudo chown $(id -u):$(id -g) $HOME/.kube/config

Run 'kubectl get nodes' to see this node join the cluster.

8) join worker nodes
sudo kubeadm join 192.168.2.213:6443 --token 2l36sg.h3mpxhplgzngdwoi \
	--discovery-token-ca-cert-hash sha256:c86917e89701dc7c202764131d737412c339604999435c116667a8f3eb46a009

9) create pod network

run on control plane
kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml

10) cluster working :)

#########################K8s etcd####################################
https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/
https://etcd.io/docs/v3.6/op-guide/recovery/#restoring-a-cluster
https://etcd.io/docs/v3.5/op-guide/maintenance/

***ETCD member list from etcd container
kubectl exec etcd-controlplane -n kube-system -- sh -c "ETCDCTL_API=3 etcdctl --endpoints=127.0.0.1:2379 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key member list"
585d80bb4063b7b5, started, controlplane, https://192.168.2.111:2380, https://192.168.2.111:2379, false
78b3ef7922ed2c96, started, controlplane02, https://192.168.2.110:2380, https://192.168.2.110:2379, false

***ETCD member status from etcd container
 kubectl exec etcd-controlplane -n kube-system -- sh -c "ETCDCTL_API=3 etcdctl --cluster=true --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key --write-out=table endpoint status"
+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
|          ENDPOINT          |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |
+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
| https://192.168.2.111:2379 | 585d80bb4063b7b5 |  3.5.15 |  2.1 MB |      true |      false |         2 |     140771 |             140771 |        |
| https://192.168.2.110:2379 | 78b3ef7922ed2c96 |  3.5.15 |  2.1 MB |     false |      false |         2 |     140771 |             140771 |        |
+----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+

***ETCD cluster health from etcd container
kubectl exec etcd-controlplane -n kube-system -- sh -c "ETCDCTL_API=3 etcdctl --cluster=true --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key --write-out=table endpoint health"
+----------------------------+--------+------------+-------+
|          ENDPOINT          | HEALTH |    TOOK    | ERROR |
+----------------------------+--------+------------+-------+
| https://192.168.2.111:2379 |   true | 9.022405ms |       |
| https://192.168.2.110:2379 |   true | 9.146941ms |       |
+----------------------------+--------+------------+-------+

***ETCD cluster hash status from etcd container
kubectl exec etcd-controlplane -n kube-system -- sh -c "ETCDCTL_API=3 etcdctl --cluster=true --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key --write-out=table endpoint hashkv"
+----------------------------+-----------+
|          ENDPOINT          |   HASH    |
+----------------------------+-----------+
| https://192.168.2.111:2379 | 647937148 |
| https://192.168.2.110:2379 | 647937148 |
+----------------------------+-----------+

*** run from client outside of etcd container

ETCDCTL_API=3 sudo etcdctl --endpoints=192.168.2.111:2379 --cluster=true --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key --write-out=table endpoint hashkv 
+----------------------------+------------+
|          ENDPOINT          |    HASH    |
+----------------------------+------------+
| https://192.168.2.111:2379 | 3750463371 |
| https://192.168.2.110:2379 | 3750463371 |
+----------------------------+------------+

*** Taking etcd cluster snapshot.
ETCDCTL_API=3 sudo etcdctl --endpoints=192.168.2.111:2379 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key snapshot save /tmp/snapshot-01062025.db
{"level":"info","ts":1736191163.313979,"caller":"snapshot/v3_snapshot.go:119","msg":"created temporary db file","path":"/tmp/snapshot-01062025.db.part"}
{"level":"info","ts":"2025-01-06T19:19:23.32106Z","caller":"clientv3/maintenance.go:212","msg":"opened snapshot stream; downloading"}
{"level":"info","ts":1736191163.3212695,"caller":"snapshot/v3_snapshot.go:127","msg":"fetching snapshot","endpoint":"192.168.2.111:2379"}
{"level":"info","ts":"2025-01-06T19:19:23.33989Z","caller":"clientv3/maintenance.go:220","msg":"completed snapshot read; closing"}
{"level":"info","ts":1736191163.3493383,"caller":"snapshot/v3_snapshot.go:142","msg":"fetched snapshot","endpoint":"192.168.2.111:2379","size":"2.1 MB","took":0.035198544}
{"level":"info","ts":1736191163.3494608,"caller":"snapshot/v3_snapshot.go:152","msg":"saved","path":"/tmp/snapshot-01062025.db"}
Snapshot saved at /tmp/snapshot-01062025.db

*** Checking snapshot status
ETCDCTL_API=3 sudo etcdctl --endpoints=192.168.2.111:2379 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key snapshot --write-out=table status /tmp/snapshot-01062025.db
+----------+----------+------------+------------+
|   HASH   | REVISION | TOTAL KEYS | TOTAL SIZE |
+----------+----------+------------+------------+
| 7a7a5d68 |   127606 |        857 |     2.1 MB |
+----------+----------+------------+------------+


**** adding another controler plane to cluster***************************
sudo kubeadm join 192.168.2.213:6443 --token 2l36sg.h3mpxhplgzngdwoi \
	--discovery-token-ca-cert-hash sha256:c86917e89701dc7c202764131d737412c339604999435c116667a8f3eb46a009 \
	--control-plane --certificate-key 0465e13cefb6e34182875b57581a2e2e4a2d25576fb642aa10b70c0ab4376720

join new controller to cluster after installing and setting up.  you may need to regenerate key depending how much time has passed

*************************************************************************


##########setting up k8s external loadbalancer#################
create a nginx loadbalancer:
https://cloudinfrastructureservices.co.uk/nginx-load-balancing/

apt-get install nginx -y
systemctl start nginx 
systemctl enable nginx
################################/etc/nginx/conf.d/loadbalancer.conf#################################
upstream backend {
	server 192.168.2.112:30008;
	server 192.168.2.113:30008;
   }

   server {
       listen	80;
       server_name 192.168.2.213;


       location / {
	   proxy_redirect	off;
	   proxy_set_header	X-Real-IP $remote_addr;
	   proxy_set_header	X-Forward-For $proxy_add_x_forwarded_for;
	   proxy_set_header	Host $http_host;
       proxy_pass http://backend;
    }

}  
#####################################################################################################

!!!!! adding new worker node to k8s a while after initial build!!!!!!!!!!!!!!!!
**** if you get 'Retrying due to error: could not find a JWS signature in the cluster-info ConfigMap for token ID "2l36sg"' when running
sudo kubeadm join 192.168.2.213:6443 --token 2l36sg.h3mpxhplgzngdwoi --discovery-token-ca-cert-hash sha256:c86917e89701dc7c202764131d737412c339604999435c116667a8f3eb46a009 --v=5
**** recreate token and retry join
kubeadm token create
i2w6gn.kgbl77drj3ro70kb
kwood@controlplane:~$ kubeadm token list
TOKEN                     TTL         EXPIRES                USAGES                   DESCRIPTION   EXTRA GROUPS
i2w6gn.kgbl77drj3ro70kb   23h         2025-02-17T16:43:53Z   authentication,signing   <none>    system:bootstrappers:kubeadm:default-node-token

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!


############setting up nginx ingress controller#########################
https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/baremetal/deploy.yaml
namespace/ingress-nginx created
serviceaccount/ingress-nginx created
serviceaccount/ingress-nginx-admission created
role.rbac.authorization.k8s.io/ingress-nginx created
role.rbac.authorization.k8s.io/ingress-nginx-admission created
clusterrole.rbac.authorization.k8s.io/ingress-nginx created
clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission created
rolebinding.rbac.authorization.k8s.io/ingress-nginx created
rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx created
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created
configmap/ingress-nginx-controller created
service/ingress-nginx-controller created
service/ingress-nginx-controller-admission created
deployment.apps/ingress-nginx-controller created
job.batch/ingress-nginx-admission-create created
job.batch/ingress-nginx-admission-patch created
ingressclass.networking.k8s.io/nginx created
validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission created

ssl pass through:
https://arunsworld.medium.com/ssl-passthrough-via-kubernetes-ingress-b3eaf3c7c9da


      listen  443 ssl;
       server_name *.apexkube.xyz

30459

1)  create a namespace to deploy ingress-controller
    kubectl create namespace ingress-space

2)  create config map for nginx configuration in "ingress-space" namespace
    kubectl create configmap nginx-configuration -n ingress-space

3)  create service account in the ingress-spaces. required for ingress-controller
    kubectl create serviceaccount ingress-serviceaccount -n ingress-space

4)  create roles/role-bindings for service account related to ingress-controller
    Resources: configmaps(get,create),configmaps(get,update [ingress-controller-leader-nginx]),endpoints(get),namespaces(get),pods(get),secrets(get)

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: ingress-space
  name: ingress-role
rules:
- apiGroups: [""] 
  resources: ["configmaps"]
  verbs: ["get", "create"]
- apiGroups: [""]
  resources: ["configmaps"]
  resourceNames: ["ingress-controller-leader-nginx"]
  verbs: ["get", "update"]
- apiGroups: [""]
  resources: ["endpoints"]
  verbs: ["get"]
- apiGroups: [""]
  resources: ["namespaces"]
  verbs: ["get"]
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get"]
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["get"]
 
---
# This role binding allows "jane" to read pods in the "default" namespace.
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: binding-ingress
  namespace: ingress-space
subjects:
- kind: User
  name: ingress-serviceaccount # Name is case sensitive
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role #this must be Role or ClusterRole
  name: ingress-role # this must match the name of the Role or ClusterRole you wish to bind to
  apiGroup: rbac.authorization.k8s.io



5) Deployment of controller pod


kwood@controlplane:~/lab-work$ more ingress-controller.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-ingress-controller
  namespace: ingress-space
spec:
  replicas: 1
  selector:
    matchLabels:
      name: nginx-ingress
  template:
    metadata:
      labels:
        name: nginx-ingress
    spec:
      serviceAccountName: ingress-serviceaccount
      containers:
        - name: nginx-ingress-controller
          image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.21.0
          args:
            - /nginx-ingress-controller
            - --configmap $(POD_NAMESPACE)/nginx-configuration
       
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
              
          ports:
            - name: http
              containerPort: 80
            - name: https
              containerPort: 443    


6) need to expost the nginx controller with a service node port

kubectl expose deploy nginx-ingress-controller -n ingress-space --name ingress --port=80 --target-port=80 --type NodePort
****cannot setup node port with this command need to do a edit after
kubectl edit svc ingress -n ingress-space  -> change nodePort to 30080


kwood@controlplane:~/lab-work$ cat ingress-controller-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: ingress
  namepace: ingress-space
spec:
  type: NodePort
  ports:
  - port: 80
    targetPort: 80
    protocol: TCP
    name: http
  - port: 443
    targetPort: 443
    protocol: TCP
    name: https
  selector:
    name: nginx-ingress



7) create a ingress resource is a set of configuration and rules applied to controller. route traffic base on url . etc..ingress resource is created with a kubernetes definition file

#####ingress-resource.yaml#####
apiVersion: extensions/v1beta1
kind: ingress
metadata:
  name: ingress-test
spec:
 
 rules:
 - host: www.woodez.org
   http:
     paths:     
     - backend:
         serviceName: nginx-service
         servicePort: 80

#############################################


###############creating a self signed cert for app nginx
https://www.humankode.com/ssl/create-a-selfsigned-certificate-for-nginx-in-5-minutes/
https://betterstack.com/community/questions/getting-chrome-to-accept-self-signed-localhost-certificate/
#########################################################

https://medium.com/@martin.hodges/simple-service-for-testing-kubernetes-configurations-189e9409b776
https://sam-thomas.medium.com/kubernetes-ingressclass-error-ingress-does-not-contain-a-valid-ingressclass-78aab72c15a6
https://github.com/kubernetes/ingress-nginx/blob/main/docs/troubleshooting.md
https://kubernetes.io/docs/reference/kubectl/generated/kubectl_create/kubectl_create_ingress/



#########################Building a 3 node Ceph cluster#############################
Nodes in ceph cluster:
192.168.2.213 - loadbalancer (lvm created) 
192.168.2.110 - controlplane02
192.168.2.114 - controlplane03 

Adding disk to node:
cd /var/lib/libvirt/images
sudo qemu-img create -f raw controlplane02-disk2-20G 20G
sudo chown libvirt-qemu:kvm controlplane02-disk2-20G
sudo qemu-img create -f raw controlplane03-disk2-20G 20G
sudo chown libvirt-qemu:kvm controlplane03-disk2-20G
sudo qemu-img create -f raw loadbalancer-disk2-20G 20G
sudo chown libvirt-qemu:kvm loadbalancer-disk2-20G
sudo qemu-img create -f raw node03-disk2-20G 20G
sudo chown libvirt-qemu:kvm node03-disk2-20G
sudo qemu-img create -f raw node04-disk2-20G 20G
sudo chown libvirt-qemu:kvm node04-disk2-20G

sudo qemu-img create -f raw node01-disk2-20G 20G
sudo chown libvirt-qemu:kvm node01-disk2-20G
sudo qemu-img create -f raw node02-woodez-disk2-20G 20G
sudo chown libvirt-qemu:kvm node02-woodez-disk2-20G
**********ATTACH DISK to VM*********************
sudo virsh attach-disk loadbalancer \
--source /var/lib/libvirt/images/loadbalancer-disk2-20G \
--target vdb \
--persistent

sudo virsh attach-disk controlplane02 \
--source /var/lib/libvirt/images/controlplane02-disk2-20G \
--target vdb \
--persistent

sudo virsh attach-disk controlplane03 \
--source /var/lib/libvirt/images/controlplane03-disk2-20G \
--target vdb \
--persistent

sudo virsh attach-disk controlplane01 \
--source /var/lib/libvirt/images/controlplane01-disk2-20G \
--target vdb \
--persistent

sudo virsh attach-disk node01 \
--source /var/lib/libvirt/images/node01-disk2-20G \
--target vdb \
--persistent

sudo virsh attach-disk node02-woodez \
--source /var/lib/libvirt/images/node02-woodez-disk2-20G \
--target vdb \
--persistent

sudo virsh attach-disk node03 \
--source /var/lib/libvirt/images/node03-disk2-20G \
--target vdb \
--persistent

sudo virsh attach-disk node04 \
--source /var/lib/libvirt/images/node04-disk2-20G \
--target vdb \
--persistent

************************************************

**************DETACH DISK**********************
sudo virsh detach-disk loadbalancer \
/var/lib/libvirt/images/loadbalancer-disk2-20G \
--persistent \
--config \
--live

sudo virsh detach-disk node03 \
/var/lib/libvirt/images/node03-disk2-40G \
--persistent \
--config \
--live

sudo virsh detach-disk node02-woodez \
/var/lib/libvirt/images/node02-woodez-disk2-20G \
--persistent \
--config \
--live
**********************************************

Once physical disk is attached create logical volume(leave raw):  ---> !!!!!!!!Not needed ceph can do this!!!!!!!!!!!!!
sudo pvcreate /dev/vdb
sudo vgcreate loadbalancer_vg /dev/vdb
sudo vgcreate controlplane02_vg /dev/vdb
sudo vgcreate controlplane03_vg /dev/vdb
sudo lvcreate -l +100%FREE -n loadbalancer_vol1 loadbalancer_vg
sudo lvcreate -l +100%FREE -n controlplane02_vol1 controlplane02_vg
sudo lvcreate -l +100%FREE -n controlplane03_vol1 controlplane03_vg

 --- Logical volume ---
  LV Path                /dev/loadbalancer_vg/loadbalancer_vol1
  LV Name                loadbalancer_vol1
  VG Name                loadbalancer_vg

*****************remove lvm**************************
lvremove /dev/loadbalancer_vg/loadbalancer_vol1
vgremove loadbalancer_vg
pvremove /dev/vdb
*****************************************************

**************Uninstall cifs on all nodes:**************************
systemctl stop ceph-mon.target
systemctl stop ceph-mgr.target
systemctl stop ceph-mds.target
systemctl stop ceph-osd.target
rm -rf /etc/systemd/system/ceph*
killall -9 ceph-mon ceph-mgr ceph-mds
rm -rf /var/lib/ceph/mon/  /var/lib/ceph/mgr/  /var/lib/ceph/mds/
pveceph purge
apt purge ceph-mon ceph-osd ceph-mgr ceph-mds
apt purge ceph-base ceph-mgr-modules-core
rm -rf /etc/ceph/*
rm -rf /etc/pve/ceph.conf
rm -rf /etc/pve/priv/ceph.*
********************************************************************



******************create ceph cluster****************************
https://www.server-world.info/en/note?os=Ubuntu_24.04&p=ceph&f=1
https://medium.com/@kayvan.sol2/install-ceph-cluster-by-cephadm-169829ebf531
https://rook.io/docs/rook/latest/Storage-Configuration/Advanced/ceph-osd-mgmt/ -> OSD MANAGEMENT

Generating a dashboard self-signed certificate...
Creating initial admin user...
Fetching dashboard port number...
Ceph Dashboard is now available at:

	     URL: https://loadbalancer:8443/
	    User: admin
	Password: 123456

Enabling client.admin keyring and conf on hosts with "admin" label
Saving cluster configuration to /var/lib/ceph/19492a45-e7e7-11ef-82db-5254000002cb/config directory
You can access the Ceph CLI as following in case of multi-cluster or non-default config:

	sudo /usr/sbin/cephadm shell --fsid 19492a45-e7e7-11ef-82db-5254000002cb -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring

Or, if you are only running a single cluster on this host:

	sudo /usr/sbin/cephadm shell 

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/en/latest/mgr/telemetry/

Bootstrap complete.


root@loadbalancer:~# cephadm shell --fsid 19492a45-e7e7-11ef-82db-5254000002cb
Inferring config /var/lib/ceph/19492a45-e7e7-11ef-82db-5254000002cb/mon.loadbalancer/config
This is a development version of cephadm.
For information regarding the latest stable release:
    https://docs.ceph.com/docs/squid/cephadm/install
root@loadbalancer:/# 
*************************************************************************************
#############################Installing Rook#########################################
https://youtu.be/IGj__IRc7J8?si=_X8FX6ws3daawP1G
https://rook.io/docs/rook/latest-release/Getting-Started/intro/
https://youtu.be/98QujsS7jFI?si=UhLH5bd7YcQnjxcN
https://www.server-world.info/en/note?os=Ubuntu_24.04&p=ceph&f=2

****************************Removing rook********************************************
sudo kubectl delete all --all -n rook-ceph
sudo kubectl api-resources --verbs=list --namespaced -o name | xargs -n 1 sudo kubectl get --show-kind --ignore-not-found -n rook-ceph

!!!!!!!!!operator.yaml change!!!!!!!!!!!!!!!
ROOK_ENABLE_DISCOVERY_DAEMON: "true"   -> automatically detects newly added disks
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

!!!!!!!!!cluster.yaml change!!!!!!!!!!!!!!!
provider: host    -> so cephs cluster is available outside of k8s cluster
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

!!!!!!!!!!Get admin password!!!!!!!!!!!!!!!
sudo kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath="{['data']['password']}" | base64 --decode && echo
b7!r=-|SJ(F!JQjOT+.K
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

!!!!!!!!!!Launch UI cephs!!!!!!!!!!!!!!!!!!
kubectl port-forward svc/rook-ceph-mgr-dashboard -n rook-ceph 31515:8443
https://127.0.0.1:31515
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

!!!!!!!!!!Connect to toolbox to ceph cli!!!!!!!!!!!!!!!
kubectl -n rook-ceph exec -it deploy/rook-ceph-tools -- bash
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

!!!!!!!!!! Testing my new ceph rook storage !!!!!!!
*****cat storageclass-woodez.yaml********
apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: replicapool
  namespace: rook-ceph # namespace:cluster
spec:
  failureDomain: osd
  replicated:
    size: 1
    # Disallow setting pool with replica 1, this could lead to data loss without recovery.
    # Make sure you're *ABSOLUTELY CERTAIN* that is what you want
    requireSafeReplicaSize: false
    # gives a hint (%) to Ceph in terms of expected consumption of the total cluster capacity of a given pool
    # for more info: https://docs.ceph.com/docs/master/rados/operations/placement-groups/#specifying-expected-pool-size
    #targetSizeRatio: .5
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rook-ceph-block
provisioner: rook-ceph.rbd.csi.ceph.com # csi-provisioner-name
parameters:
  # clusterID is the namespace where the rook cluster is running
  # If you change this namespace, also change the namespace below where the secret namespaces are defined
  clusterID: rook-ceph # namespace:cluster

  # If you want to use erasure coded pool with RBD, you need to create
  # two pools. one erasure coded and one replicated.
  # You need to specify the replicated pool here in the `pool` parameter, it is
  # used for the metadata of the images.
  # The erasure coded pool must be set as the `dataPool` parameter below.
  #dataPool: ec-data-pool
  pool: replicapool

  # RBD image format. Defaults to "2".
  imageFormat: "2"

  # RBD image features. Available for imageFormat: "2". CSI RBD currently supports only `layering` feature.
  imageFeatures: layering

  # The secrets contain Ceph admin credentials. These are generated automatically by the operator
  # in the same namespace as the cluster.
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph # namespace:cluster
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph # namespace:cluster
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph # namespace:cluster
  # Specify the filesystem type of the volume. If not specified, csi-provisioner
  # will set default as `ext4`.
  csi.storage.k8s.io/fstype: ext4
# uncomment the following to use rbd-nbd as mounter on supported nodes
#mounter: rbd-nbd
allowVolumeExpansion: true
reclaimPolicy: Delete

*************cat test-volumeclaim.yaml************** 
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: woodez-claim
  namespace: woodez
spec:
  accessModes:
     - ReadWriteOnce
  storageClassName: rook-ceph-block
  resources:
     requests:
       storage: 500Mi

***************cat test-pod.yaml********************
apiVersion: v1
kind: Pod
metadata:
  name: brook-test
  namespace: woodez
spec:
  containers:
    - name: myfrontend
      image: nginx
      volumeMounts:
      - mountPath: "/var/www/html"
        name: mypd
  volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: woodez-claim

Kevins-MacBook-Air:storageclass kwood$ kubectl get sc
NAME              PROVISIONER                  RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
rook-ceph-block   rook-ceph.rbd.csi.ceph.com   Delete          Immediate           true                   79m


Kevins-MacBook-Air:storageclass kwood$ kubectl get persistentvolumeclaim -n woodez
NAME           STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      VOLUMEATTRIBUTESCLASS   AGE
woodez-claim   Bound    pvc-5cfcb0d4-cb3b-4662-bff7-41b4a23aa0bf   500Mi      RWO            rook-ceph-block   <unset>                 17m

kubectl -n woodez exec -it brook-test -- bash
root@brook-test:/# cd /var/www/html/
root@brook-test:/var/www/html# df -k .
Filesystem     1K-blocks  Used Available Use% Mounted on
/dev/rbd0         469328    14    465218   1% /var/www/html
root@brook-test:/var/www/html# df -h .
Filesystem      Size  Used Avail Use% Mounted on
/dev/rbd0       459M   14K  455M   1% /var/www/html

#############################################################Installing mysql on woodez k8s####################################################

1) install helm on MacBook
https://helm.sh/docs/intro/install/
https://argo-cd.readthedocs.io/en/stable/user-guide/helm/
brew install helm

helm create <name>                      # Creates a chart directory along with the common files and directories used in a chart.
helm package <chart-path>               # Packages a chart into a versioned chart archive file.
helm lint <chart>                       # Run tests to examine a chart and identify possible issues:
helm show all <chart>                   # Inspect a chart and list its contents:
helm show values <chart>                # Displays the contents of the values.yaml file
helm pull <chart>                       # Download/pull chart 
helm pull <chart> --untar=true          # If set to true, will untar the chart after downloading it
helm pull <chart> --verify              # Verify the package before using it
helm pull <chart> --version <number>    # Default-latest is used, specify a version constraint for the chart version to use
helm dependency list <chart>            # Display a list of a chartâ€™s dependencies:

helm install <name> <chart>                           # Install the chart with a name
helm install <name> <chart> --namespace <namespace>   # Install the chart in a specific namespace
helm install <name> <chart> --set key1=val1,key2=val2 # Set values on the command line (can specify multiple or separate values with commas)
helm install <name> <chart> --values <yaml-file/url>  # Install the chart with your specified values
helm install <name> <chart> --dry-run --debug         # Run a test installation to validate chart (p)
helm install <name> <chart> --verify                  # Verify the package before using it 
helm install <name> <chart> --dependency-update       # update dependencies if they are missing before installing the chart
helm uninstall <name>                                 # Uninstall a release

helm upgrade <release> <chart>                            # Upgrade a release
helm upgrade <release> <chart> --atomic                   # If set, upgrade process rolls back changes made in case of failed upgrade.
helm upgrade <release> <chart> --dependency-update        # update dependencies if they are missing before installing the chart
helm upgrade <release> <chart> --version <version_number> # specify a version constraint for the chart version to use
helm upgrade <release> <chart> --values                   # specify values in a YAML file or a URL (can specify multiple)
helm upgrade <release> <chart> --set key1=val1,key2=val2  # Set values on the command line (can specify multiple or separate valuese)
helm upgrade <release> <chart> --force                    # Force resource updates through a replacement strategy
helm rollback <release> <revision>                        # Roll back a release to a specific revision
helm rollback <release> <revision>  --cleanup-on-fail     # Allow deletion of new resources created in this rollback when rollback fails

helm repo add <repo-name> <url>   # Add a repository from the internet:
helm repo list                    # List added chart repositories
helm repo update                  # Update information of available charts locally from chart repositories
helm repo remove <repo_name>      # Remove one or more chart repositories
helm repo index <DIR>             # Read the current directory and generate an index file based on the charts found.
helm repo index <DIR> --merge     # Merge the generated index with an existing index file
helm search repo <keyword>        # Search repositories for a keyword in charts
helm search hub <keyword>         # Search for charts in the Artifact Hub or your own hub instance

helm list                       # Lists all of the releases for a specified namespace, uses current namespace context if namespace not specified
helm list --all                 # Show all releases without any filter applied, can use -a
helm list --all-namespaces      # List releases across all namespaces, we can use -A
helm list -l key1=value1,key2=value2 # Selector (label query) to filter on, supports '=', '==', and '!='
helm list --date                # Sort by release date
helm list --deployed            # Show deployed releases. If no other is specified, this will be automatically enabled
helm list --pending             # Show pending releases
helm list --failed              # Show failed releases
helm list --uninstalled         # Show uninstalled releases (if 'helm uninstall --keep-history' was used)
helm list --superseded          # Show superseded releases
helm list -o yaml               # Prints the output in the specified format. Allowed values: table, json, yaml (default table)
helm status <release>           # This command shows the status of a named release.
helm status <release> --revision <number>   # if set, display the status of the named release with revision
helm history <release>          # Historical revisions for a given release.
helm env                        # Env prints out all the environment information in use by Helm.

helm get all <release>      # A human readable collection of information about the notes, hooks, supplied values, and generated manifest file of the given release.
helm get hooks <release>    # This command downloads hooks for a given release. Hooks are formatted in YAML and separated by the YAML '---\n' separator.
helm get manifest <release> # A manifest is a YAML-encoded representation of the Kubernetes resources that were generated from this release's chart(s). If a chart is dependent on other charts, those resources will also be included in the manifest.
helm get notes <release>    # Shows notes provided by the chart of a named release.
helm get values <release>   # Downloads a values file for a given release. use -o to format output

helm plugin install <path/url1>     # Install plugins
helm plugin list                    # View a list of all installed plugins
helm plugin update <plugin>         # Update plugins
helm plugin uninstall <plugin>      # Uninstall a plugin


2) install mysql test
https://8grams.medium.com/how-to-install-database-mysql-and-postgresql-on-kubernetes-the-better-way-d9524d710be8

a) kubectl create namespace woodez-database

b) Kevins-MacBook-Air:woodez-database kwood$ cat database-volumeclaim.yaml 
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: database-claim
  namespace: woodez-database
spec:
  accessModes:
     - ReadWriteMany
  storageClassName: rook-ceph-block
  resources:
     requests:
       storage: 10Gi

Kevins-MacBook-Air:woodez-database kwood$ kubectl apply -f database-volumeclaim.yaml
persistentvolumeclaim/database-claim created
Kevins-MacBook-Air:woodez-database kwood$ kubectl get pvc -n woodez-database
NAME             STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      VOLUMEATTRIBUTESCLASS   AGE
database-claim   Bound    pvc-ee66b0f7-44cb-4028-913b-cfc1e27434fd   10Gi       RWO            rook-ceph-block   <unset>                 6s


helm install mysql -f mysql-values.yaml bitnami/mysql -n woodez-database


Kevins-MacBook-Air:woodez-database-mysql kwood$ helm install mysql -f mysql-values.yaml bitnami/mysql -n woodez-database
NAME: mysql
LAST DEPLOYED: Wed Feb 19 11:12:18 2025
NAMESPACE: woodez-database
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
CHART NAME: mysql
CHART VERSION: 12.2.2
APP VERSION: 8.4.4

Did you know there are enterprise versions of the Bitnami catalog? For enhanced secure software supply chain features, unlimited pulls from Docker, LTS support, or application customization, see Bitnami Premium or Tanzu Application Catalog. See https://www.arrow.com/globalecs/na/vendors/bitnami for more information.

** Please be patient while the chart is being deployed **

Tip:

  Watch the deployment status using the command: kubectl get pods -w --namespace woodez-database

Services:

  echo Primary: mysql.woodez-database.svc.cluster.local:3306

Execute the following to get the administrator credentials:

  echo Username: root
  MYSQL_ROOT_PASSWORD=$(kubectl get secret --namespace woodez-database mysql -o jsonpath="{.data.mysql-root-password}" | base64 -d)

To connect to your database:

  1. Run a pod that you can use as a client:

      kubectl run mysql-client --rm --tty -i --restart='Never' --image  docker.io/bitnami/mysql:8.0.32 --namespace woodez-database --env MYSQL_ROOT_PASSWORD=$MYSQL_ROOT_PASSWORD --command -- bash

  2. To connect to primary service (read/write):

      mysql -h mysql.woodez-database.svc.cluster.local -uroot -p"$MYSQL_ROOT_PASSWORD"





WARNING: Rolling tag detected (bitnami/mysql:8.0.32), please note that it is strongly recommended to avoid using rolling tags in a production environment.
+info https://techdocs.broadcom.com/us/en/vmware-tanzu/application-catalog/tanzu-application-catalog/services/tac-doc/apps-tutorials-understand-rolling-tags-containers-index.html

WARNING: There are "resources" sections in the chart not set. Using "resourcesPreset" is not recommended for production. For production installations, please set the following values according to your workload needs:
  - primary.resources
  - secondary.resources
  - volumePermissions.resources
+info https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/

âš  SECURITY WARNING: Original containers have been substituted. This Helm chart was designed, tested, and validated on multiple platforms using a specific set of Bitnami and Tanzu Application Catalog containers. Substituting other containers is likely to cause degraded security and performance, broken chart features, and missing environment variables.

Substituted images detected:
  - docker.io/bitnami/mysql:8.0.32

âš  WARNING: Original containers have been retagged. Please note this Helm chart was tested, and validated on multiple platforms using a specific set of Tanzu Application Catalog containers. Substituting original image tags could cause unexpected behavior.

Retagged images:
  - docker.io/bitnami/mysql:8.0.32

************remove not ready node from k8s cluster****************************
kubectl drain --ignore-daemonsets node-rasp01
kubectl delete node node-rasp01
******************************************************************************









